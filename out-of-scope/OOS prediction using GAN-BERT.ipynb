{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69603c19ebb94c6098ac8cb128b13993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b8deef7718b4f848674a15590ab6d12",
              "IPY_MODEL_8d7134789ec44ea394eab128aeb34afc",
              "IPY_MODEL_0f160ffb85e54796818ae5ba1725da98"
            ],
            "layout": "IPY_MODEL_a4f10cadeecb4702913f2d187f242748"
          }
        },
        "3b8deef7718b4f848674a15590ab6d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_633b5cf1e8094b4db6b81a38f2c41338",
            "placeholder": "​",
            "style": "IPY_MODEL_aed5938fccb8427c99ced2d4a5702d23",
            "value": "100%"
          }
        },
        "8d7134789ec44ea394eab128aeb34afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0013fde3de7142a89d639d2ef67b5ad2",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f4f4d2fe1dd4a6b9bcfc4e9c738134b",
            "value": 3
          }
        },
        "0f160ffb85e54796818ae5ba1725da98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c43a684d23b147378018ec95e3508abc",
            "placeholder": "​",
            "style": "IPY_MODEL_395a2d8691e04e609c50031604637d4c",
            "value": " 3/3 [00:00&lt;00:00, 102.04it/s]"
          }
        },
        "a4f10cadeecb4702913f2d187f242748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633b5cf1e8094b4db6b81a38f2c41338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed5938fccb8427c99ced2d4a5702d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0013fde3de7142a89d639d2ef67b5ad2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f4f4d2fe1dd4a6b9bcfc4e9c738134b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c43a684d23b147378018ec95e3508abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "395a2d8691e04e609c50031604637d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Out-of-scope Prediction with GAN-BERT\n",
        "\n",
        "In this notebook, we use the OOS+ variant of the CLINC150 dataset which contains 250 training utterances of the OOS class. \n",
        "We use the GAN-BERT model for the prediction of this oos class (label 42 in the huggingface dataset we use) and see how its performance compares to BERT-base, which the CLINC150 paper had tested on."
      ],
      "metadata": {
        "id": "JdBQ2DcpYRek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary packages"
      ],
      "metadata": {
        "id": "rItvsdM7Tgk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.3.2\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOAcH6FiUDuA",
        "outputId": "6a43f87a-3964-4c74-c1d3-b165872d6be8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.3.2\n",
            "  Downloading transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (3.8.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.3.2) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=d8e4d97f1544a9bedbc8d7051b0bb34b5707a97b4562e5c66268105e063912a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[K     |████████████████████████████████| 452 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 9.9 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yzDP5FZslqii"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "Tj6Wy23MpCLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "JdliPXikUQu3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjQpweKxUU5N",
        "outputId": "8baaa984-59d4-447f-aea7-e864b9f0049b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "MjAspE2Ywj1X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing "
      ],
      "metadata": {
        "id": "53cSfHTmpE2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the huggingface dataset for this exercise of seeing how well GAN-BERT performs in identifying out-of-scope examples.\n",
        "\n",
        "For the previous exercise of in-scope prediction using different variations of labeled and unlabeled data, we used the 'FULL' variant of the CLINC150 as obtained from their repository. \n",
        "However, for this exercise, we have decided to load the 'OOS+' variant from huggingface for 2 reasons:\n",
        "1. The OOS+ variant has more number of OOS examples as compared to the FULL dataset. The former contains 250 training examples whereas the latter contains only 100 training examples. \n",
        "2. The CLINC150 paper shows that the performance of BERT-base for out-of-scope prediction is the highest for the OOS+ dataset (as expected), hence we wanted to compare the performance of GAN-BERT with this best model. \n"
      ],
      "metadata": {
        "id": "YCJ8DCp50LRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_dataset = load_dataset(\"clinc_oos\",\"plus\")"
      ],
      "metadata": {
        "id": "qNYhrDzZA7uA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "69603c19ebb94c6098ac8cb128b13993",
            "3b8deef7718b4f848674a15590ab6d12",
            "8d7134789ec44ea394eab128aeb34afc",
            "0f160ffb85e54796818ae5ba1725da98",
            "a4f10cadeecb4702913f2d187f242748",
            "633b5cf1e8094b4db6b81a38f2c41338",
            "aed5938fccb8427c99ced2d4a5702d23",
            "0013fde3de7142a89d639d2ef67b5ad2",
            "0f4f4d2fe1dd4a6b9bcfc4e9c738134b",
            "c43a684d23b147378018ec95e3508abc",
            "395a2d8691e04e609c50031604637d4c"
          ]
        },
        "outputId": "343db004-e5e0-4481-e32b-aead90187490"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset clinc_oos (/root/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69603c19ebb94c6098ac8cb128b13993"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking an oos example & how many oos are present\n",
        "oos_texts = [sent for sent in intent_dataset['train'] if sent['intent']==42]\n",
        "print(oos_texts[14])\n",
        "print(len(oos_texts))"
      ],
      "metadata": {
        "id": "bzkrxSreBRU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b75194-292b-4029-cfaf-edbba5f4c9e5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'how many planets have we discovered', 'intent': 42}\n",
            "250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenization"
      ],
      "metadata": {
        "id": "b3yHrqNdDGxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the BertTokenizer for tokenization, BertModel as the pre-trained model and BertConfig for the parameters."
      ],
      "metadata": {
        "id": "YL_MEPaRabn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True) "
      ],
      "metadata": {
        "id": "e-wFmxFgC_jS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some of the common BERT tokens\n",
        "print(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\n",
        "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows we’re doing classification\n",
        "print(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\n",
        "print(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set "
      ],
      "metadata": {
        "id": "1sdkauYEDLCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4349b9c6-d098-4822-c6b5-95ae548589f2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SEP] 102\n",
            "[CLS] 101\n",
            "[PAD] 0\n",
            "[UNK] 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 40\n",
        "\n",
        "def tokenize_datasets(data):\n",
        "  tokenized_datasets = {}\n",
        "  for collection in data: \n",
        "    tokenized_datasets[collection] = tokenizer(data[collection]['text'], padding='max_length', max_length = max_length, truncation=True, return_tensors='pt')\n",
        "    tokenized_datasets[collection]['intent'] = data[collection]['intent']\n",
        "\n",
        "    tokenized_datasets[collection]['tokens'] = [[\"[CLS]\"] + tokenizer.tokenize(row) + [\"[SEP]\"] for row in data[collection]['text']]\n",
        "  \n",
        "  return tokenized_datasets"
      ],
      "metadata": {
        "id": "ivx9UDKNDNeQ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tokenize the dataset by converting the sentences to tokens as well as input ids. And special tokens of [CLS] and [SEP] are appending to the beginning and end of the sequence as needed by BERT. "
      ],
      "metadata": {
        "id": "HBYiFah2amks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenize_datasets(intent_dataset)"
      ],
      "metadata": {
        "id": "UAimgN-_EA7p"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens:\", tokenized_datasets['train']['tokens'][0])\n",
        "print(len(tokenized_datasets['train']['tokens'][0]))\n",
        "print(\"Input Ids:\", tokenized_datasets['train']['input_ids'][0])\n",
        "print(tokenized_datasets['train']['input_ids'][0].size())\n",
        "print(\"Attention Mask:\", tokenized_datasets['train']['attention_mask'][0])\n",
        "print(\"Intent:\", tokenized_datasets['train']['intent'][0])"
      ],
      "metadata": {
        "id": "EnF1TNP4EDIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232a5fc4-836f-4c17-f6b6-d49a84b06765"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['[CLS]', 'what', 'expression', 'would', 'i', 'use', 'to', 'say', 'i', 'love', 'you', 'if', 'i', 'were', 'an', 'italian', '[SEP]']\n",
            "17\n",
            "Input Ids: tensor([ 101, 2054, 3670, 2052, 1045, 2224, 2000, 2360, 1045, 2293, 2017, 2065,\n",
            "        1045, 2020, 2019, 3059,  102,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n",
            "torch.Size([40])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Intent: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets['train'].keys()"
      ],
      "metadata": {
        "id": "MPJ1sM52Gubr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27c6a0e-946b-4913-a5b6-12caf824a5c3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'intent', 'tokens'])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the max sentence length for padding\n",
        "max_len = 0\n",
        "for idx, sent in enumerate(tokenized_datasets['train']['tokens']):\n",
        "  if len(sent) > max_len:\n",
        "    max_len = len(sent)\n",
        "    max_sent_idx = idx\n",
        "\n",
        "print(max_len)\n",
        "print(max_sent_idx)\n",
        "print(tokenized_datasets['train']['tokens'][max_sent_idx])"
      ],
      "metadata": {
        "id": "-j0u2JP6HVBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbe46e0-0835-42d6-f9b1-7e4515cadaab"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "4569\n",
            "['[CLS]', 'my', 'car', \"'\", 's', 'been', 'throwing', 'ce', '##ls', 'that', 'i', 'think', 'are', 'oil', 'related', ',', 'so', 'i', \"'\", 'm', 'wondering', 'if', 'it', \"'\", 's', 'time', 'to', 'take', 'it', 'in', 'or', 'not', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We look at all the sentences from the train dataset to see which is the longest sentence. This helps us perform padding for all sentences in the batch. \n",
        "Looking at the above cell, we see that the max length of a sentence is 33 hence, to round it up, 40 has been taken as the max padded length for each sequence. "
      ],
      "metadata": {
        "id": "OlNzGGBuIp3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Dataset & Dataloader"
      ],
      "metadata": {
        "id": "Vl8tx9LKI39X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we do not have any unlabeled examples here that need to be masked in the model during inference (as done in the previous experiments), all the labels have a mask of 'True'. "
      ],
      "metadata": {
        "id": "fQlr9RP4cnsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_masks_train = np.ones(len(intent_dataset['train']), dtype=bool)\n",
        "label_masks_val = np.ones(len(intent_dataset['validation']), dtype=bool)\n",
        "label_masks_test = np.ones(len(intent_dataset['test']), dtype=bool)"
      ],
      "metadata": {
        "id": "7f4Ho5utAgbR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train dataloader\n",
        "data = TensorDataset(tokenized_datasets['train']['input_ids'], tokenized_datasets['train']['attention_mask'], \n",
        "                      torch.tensor(tokenized_datasets['train']['intent']), torch.tensor(label_masks_train))\n",
        "\n",
        "sampler = RandomSampler(data)\n",
        "\n",
        "train_dataloader = DataLoader(data, sampler = sampler, batch_size = 64)"
      ],
      "metadata": {
        "id": "tXUHHXVaU996"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#val dataloader\n",
        "data = TensorDataset(tokenized_datasets['validation']['input_ids'], tokenized_datasets['validation']['attention_mask'], \n",
        "                      torch.tensor(tokenized_datasets['validation']['intent']), torch.tensor(label_masks_val))\n",
        "\n",
        "sampler = SequentialSampler(data)\n",
        "\n",
        "val_dataloader = DataLoader(data, sampler = sampler, batch_size = 64)"
      ],
      "metadata": {
        "id": "U_pj7vcIXmfF"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test dataloader\n",
        "data = TensorDataset(tokenized_datasets['test']['input_ids'], tokenized_datasets['test']['attention_mask'], \n",
        "                      torch.tensor(tokenized_datasets['test']['intent']), torch.tensor(label_masks_test))\n",
        "\n",
        "sampler = SequentialSampler(data)\n",
        "\n",
        "test_dataloader = DataLoader(data, sampler = sampler, batch_size = 64)"
      ],
      "metadata": {
        "id": "1mAZUWTfih0X"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each batch in the dataloader have the following components:\n",
        "1. Input Ids - converting tokens to numbers\n",
        "2. Attention Masks - assigning a mask of 1 for [CLS], [SEP] and tokens, and 0 for all [PAD] tokens.\n",
        "3. Label - intent as a number \n",
        "4. Label Mask - mask of 0 for unlabeled data and 1 for labeled data. "
      ],
      "metadata": {
        "id": "CAKJj52ZdFJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Model"
      ],
      "metadata": {
        "id": "p0r3sGdQdzMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT\n",
        "transformer =  BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "P0hcfDZPOYkn"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generator\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size, output_size, hidden_sizes, dropout_rate):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep"
      ],
      "metadata": {
        "id": "oD_-wMT0d_vz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_labels, dropout_rate):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ],
      "metadata": {
        "id": "xeIbz8kIeGXz"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_size = 100\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "hidden_size = config.hidden_size # BERT outputs a 768 embedding vector\n",
        "num_hidden_layers_gen = 1\n",
        "hidden_levels_gen = [hidden_size for i in range(0, num_hidden_layers_gen)]\n",
        "num_hidden_layers_disc = 1\n",
        "hidden_levels_disc = [hidden_size for i in range(0, num_hidden_layers_disc)]\n",
        "\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.2\n",
        "\n",
        "intents = [sent['intent'] for sent in intent_dataset['train']]\n",
        "num_labels = len(set(intents))\n",
        "\n",
        "generator = Generator(noise_size=noise_size, #100\n",
        "                      output_size=hidden_size, #768\n",
        "                      hidden_sizes=hidden_levels_gen, #[768] \n",
        "                      dropout_rate=out_dropout_rate) #0.2\n",
        "\n",
        "discriminator = Discriminator(input_size=hidden_size, #768\n",
        "                              hidden_sizes=hidden_levels_disc, #[768]\n",
        "                              num_labels=num_labels, #150 + 1oos\n",
        "                              dropout_rate=out_dropout_rate) #0.2\n"
      ],
      "metadata": {
        "id": "apkWunKpF5ql"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input dim - 100x768\n",
        "# output dim - 768x768\n",
        "generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTkVAX72N4jO",
        "outputId": "aef846cb-e75a-4217-c5a6-0c959dbea97c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=768, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=768, out_features=768, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input dim - 768x768\n",
        "# output dim - 768x152\n",
        "discriminator  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsBxCphHN8V4",
        "outputId": "7b7aa77e-82c9-4dcf-d602-a149a616960b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (input_dropout): Dropout(p=0.2, inplace=False)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (logit): Linear(in_features=768, out_features=152, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input dim - 30522x768\n",
        "# output dim - 768x768\n",
        "transformer "
      ],
      "metadata": {
        "id": "juw-fYOCOzSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_gpu = True\n",
        "if torch.cuda.is_available():    \n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)"
      ],
      "metadata": {
        "id": "bqaGXwp7UCvj"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#models parameters\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "learning_rate_generator = 5e-5\n",
        "learning_rate_discriminator = 5e-5\n",
        "\n",
        "#optimizer\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) "
      ],
      "metadata": {
        "id": "oC7NdupTUSq3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scheduler\n",
        "apply_scheduler = False\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "warmup_proportion = 0.1\n",
        "\n",
        "if apply_scheduler:\n",
        "  num_train_examples = len(intent_dataset['train'])\n",
        "  num_train_steps = int(num_train_examples / batch_size * num_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)"
      ],
      "metadata": {
        "id": "v96GTfibVs7M"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Evaluation"
      ],
      "metadata": {
        "id": "Rn4H3BUqkLlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats = []\n",
        "train_gen_loss_history = []\n",
        "train_disc_loss_history = []\n",
        "val_loss_history = []\n",
        "val_accuracies = []\n",
        "predictions = []\n",
        "gold_labels = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
        "  print('Training...')\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  # loss for each epoch\n",
        "  tr_g_loss = 0\n",
        "  tr_d_loss = 0\n",
        "\n",
        "  transformer.train()\n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "\n",
        "  # for idx, batch in enumerate(data_loaders['train']):\n",
        "  for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every print_each_n_step batches.\n",
        "    if idx % 40 == 0 and not idx == 0:\n",
        "        # Calculate elapsed time in minutes.\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        \n",
        "        # Report progress.\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(idx, len(train_dataloader), elapsed))\n",
        "\n",
        "    b_input_ids = batch[0].to(device) #tensor of input ids of that batch\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    b_label_mask = batch[3].to(device)\n",
        "\n",
        "    real_batch_size = b_input_ids.shape[0] \n",
        "    # print('real_batch_size: ', real_batch_size) #size of each batch = 32\n",
        "\n",
        "    model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "    hidden_states = model_outputs[-1]\n",
        "    # print('hidden states', hidden_states.size()) #32x768 - 768 per sentence in batch\n",
        "\n",
        "    noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "    # print('noise', noise.size()) # 32x100 - 100 per sentence in batch \n",
        "    gen_rep = generator(noise)\n",
        "    # print('gen output', gen_rep.size()) #32x768 \n",
        "    # both transformer + gen output are 768 size\n",
        "\n",
        "    disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "    # print('disc input', disciminator_input.size()) # 64x768\n",
        "    features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "    features_list = torch.split(features, real_batch_size) #split 64x768 into 32,32\n",
        "    D_real_features = features_list[0]\n",
        "    D_fake_features = features_list[1] \n",
        "\n",
        "    logits_list = torch.split(logits, real_batch_size)\n",
        "    D_real_logits = logits_list[0]\n",
        "    D_fake_logits = logits_list[1]  \n",
        "\n",
        "    probs_list = torch.split(probs, real_batch_size)\n",
        "    D_real_probs = probs_list[0]\n",
        "    D_fake_probs = probs_list[1]\n",
        "\n",
        "    #loss\n",
        "\n",
        "    #gen loss\n",
        "    g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + 1e-8))\n",
        "    g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "    g_loss = g_loss_d + g_feat_reg\n",
        "\n",
        "    #disc loss\n",
        "    logits = D_real_logits[:,0:-1]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    label2one_hot = torch.nn.functional.one_hot(b_labels, 151)\n",
        "    per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "    per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "    labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "    if labeled_example_count == 0:\n",
        "      D_L_Supervised = 0\n",
        "    else:\n",
        "      D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "\n",
        "    D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + 1e-8))\n",
        "    D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + 1e-8))\n",
        "    d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "    #optimization\n",
        "    gen_optimizer.zero_grad()\n",
        "    dis_optimizer.zero_grad()\n",
        "\n",
        "    g_loss.backward(retain_graph=True)\n",
        "    d_loss.backward()\n",
        "\n",
        "    gen_optimizer.step()\n",
        "    dis_optimizer.step()\n",
        "\n",
        "    tr_g_loss += g_loss.item()\n",
        "    tr_d_loss += d_loss.item()\n",
        "\n",
        "    if apply_scheduler:\n",
        "      scheduler_d.step()\n",
        "      scheduler_g.step()\n",
        "\n",
        "  avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "  avg_train_loss_d = tr_d_loss / len(train_dataloader)\n",
        "  \n",
        "  training_time = format_time(time.time() - t0)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss generator: {0:.3f}\".format(avg_train_loss_g))\n",
        "  print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "  print(\"  Training epoch took: {:}\".format(training_time))\n",
        "  train_gen_loss_history.append(avg_train_loss_g)\n",
        "  train_disc_loss_history.append(avg_train_loss_d)\n",
        "\n",
        "  #EVALUATION\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Running validation...\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  transformer.eval() \n",
        "  discriminator.eval()\n",
        "  generator.eval()\n",
        "\n",
        "  total_test_accuracy = 0\n",
        "   \n",
        "  total_test_loss = 0\n",
        "  nb_test_steps = 0\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels_ids = []\n",
        "\n",
        "  #loss\n",
        "  nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "  # for batch in data_loaders['validation']:\n",
        "  for idx, batch in enumerate(val_dataloader):\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():        \n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        _, logits, probs = discriminator(hidden_states)\n",
        "        ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "        filtered_logits = logits[:,0:-1]\n",
        "        # Accumulate the test loss.\n",
        "        total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "\n",
        "    # Accumulate the predictions and the input labels\n",
        "    _, preds = torch.max(filtered_logits, 1)\n",
        "    all_preds += preds.detach().cpu()\n",
        "    all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "  all_preds = torch.stack(all_preds).numpy()\n",
        "  all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "\n",
        "  predictions.extend(all_preds)\n",
        "  gold_labels.extend(all_labels_ids)\n",
        "\n",
        "  test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "  print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "  val_accuracies.append(test_accuracy)\n",
        "\n",
        "  # Calculate the average loss over all of the batches.\n",
        "  avg_test_loss = total_test_loss / len(val_dataloader)\n",
        "  avg_test_loss = avg_test_loss.item()\n",
        "\n",
        "  test_time = format_time(time.time() - t0)\n",
        "    \n",
        "  print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "  print(\"  Test took: {:}\".format(test_time))\n",
        "  val_loss_history.append(avg_test_loss)\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss generator': avg_train_loss_g,\n",
        "            'Training Loss discriminator': avg_train_loss_d,\n",
        "            'Valid. Loss': avg_test_loss,\n",
        "            'Valid. Accur.': test_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Test Time': test_time\n",
        "        }\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwzIwRaCWOiK",
        "outputId": "eb9e7705-5a87-4d16-f9a3-878671678406"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:11.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:36.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:00.\n",
            "\n",
            "  Average training loss generator: 0.679\n",
            "  Average training loss discriminator: 5.190\n",
            "  Training epoch took: 0:02:24\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.619\n",
            "  Test Loss: 2.627\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.778\n",
            "  Average training loss discriminator: 2.410\n",
            "  Training epoch took: 0:02:26\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.882\n",
            "  Test Loss: 0.939\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.753\n",
            "  Average training loss discriminator: 1.276\n",
            "  Training epoch took: 0:02:26\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.937\n",
            "  Test Loss: 0.461\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.737\n",
            "  Average training loss discriminator: 0.932\n",
            "  Training epoch took: 0:02:26\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.942\n",
            "  Test Loss: 0.348\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.733\n",
            "  Average training loss discriminator: 0.812\n",
            "  Training epoch took: 0:02:27\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.948\n",
            "  Test Loss: 0.322\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.731\n",
            "  Average training loss discriminator: 0.772\n",
            "  Training epoch took: 0:02:27\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.945\n",
            "  Test Loss: 0.319\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.729\n",
            "  Average training loss discriminator: 0.754\n",
            "  Training epoch took: 0:02:26\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.939\n",
            "  Test Loss: 0.347\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.730\n",
            "  Average training loss discriminator: 0.744\n",
            "  Training epoch took: 0:02:27\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.953\n",
            "  Test Loss: 0.279\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.726\n",
            "  Average training loss discriminator: 0.740\n",
            "  Training epoch took: 0:02:26\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.944\n",
            "  Test Loss: 0.329\n",
            "  Test took: 0:00:06\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    239.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    239.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    239.    Elapsed: 0:01:14.\n",
            "  Batch   160  of    239.    Elapsed: 0:01:38.\n",
            "  Batch   200  of    239.    Elapsed: 0:02:03.\n",
            "\n",
            "  Average training loss generator: 0.728\n",
            "  Average training loss discriminator: 0.736\n",
            "  Training epoch took: 0:02:27\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.941\n",
            "  Test Loss: 0.372\n",
            "  Test took: 0:00:06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "RnXPbu7bkN1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(\"Running test...\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "transformer.eval() \n",
        "discriminator.eval()\n",
        "generator.eval()\n",
        "\n",
        "total_test_accuracy = 0\n",
        "  \n",
        "total_test_loss = 0\n",
        "nb_test_steps = 0\n",
        "\n",
        "all_preds = []\n",
        "all_labels_ids = []\n",
        "\n",
        "#loss\n",
        "nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# for batch in data_loaders['validation']:\n",
        "for idx, batch in enumerate(test_dataloader):\n",
        "  b_input_ids = batch[0].to(device)\n",
        "  b_input_mask = batch[1].to(device)\n",
        "  b_labels = batch[2].to(device)\n",
        "\n",
        "  with torch.no_grad():        \n",
        "      model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "      hidden_states = model_outputs[-1]\n",
        "      _, logits, probs = discriminator(hidden_states)\n",
        "      ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "      filtered_logits = logits[:,0:-1]\n",
        "      # Accumulate the test loss.\n",
        "      total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "\n",
        "  # Accumulate the predictions and the input labels\n",
        "  _, preds = torch.max(filtered_logits, 1)\n",
        "  all_preds += preds.detach().cpu()\n",
        "  all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "all_preds = torch.stack(all_preds).numpy()\n",
        "all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "# Calculate the average loss over all of the batches.\n",
        "avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "avg_test_loss = avg_test_loss.item()\n",
        "\n",
        "test_time = format_time(time.time() - t0)\n",
        "  \n",
        "print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "print(\"  Test took: {:}\".format(test_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rBWWVtyiPDP",
        "outputId": "3ecf322f-725b-45fd-b391-678ab30ede99"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running test...\n",
            "  Accuracy: 0.875\n",
            "  Test Loss: 0.618\n",
            "  Test took: 0:00:10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "YsFFriPxGOtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking precision, recall, f1 scores\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(gold_labels, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fDuqqenFxMd",
        "outputId": "7ea51a5b-58ab-4590-ed00-6bb1e4de8db9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.84      0.90       200\n",
            "           1       0.78      0.76      0.77       200\n",
            "           2       0.93      0.83      0.88       200\n",
            "           3       0.95      0.96      0.96       200\n",
            "           4       0.91      0.94      0.93       200\n",
            "           5       0.93      0.96      0.94       200\n",
            "           6       0.97      0.82      0.89       200\n",
            "           7       0.95      0.98      0.97       200\n",
            "           8       0.90      0.94      0.92       200\n",
            "           9       0.72      0.94      0.82       200\n",
            "          10       0.90      0.90      0.90       200\n",
            "          11       0.86      0.80      0.83       200\n",
            "          12       0.90      0.96      0.93       200\n",
            "          13       0.97      0.83      0.89       200\n",
            "          14       0.87      1.00      0.93       200\n",
            "          15       0.95      0.94      0.95       200\n",
            "          16       0.97      0.76      0.85       200\n",
            "          17       0.93      0.92      0.92       200\n",
            "          18       0.92      0.97      0.95       200\n",
            "          19       0.94      0.95      0.95       200\n",
            "          20       0.79      0.97      0.87       200\n",
            "          21       1.00      0.92      0.96       200\n",
            "          22       0.91      0.90      0.91       200\n",
            "          23       0.96      0.66      0.78       200\n",
            "          24       0.96      0.95      0.95       200\n",
            "          25       0.97      0.95      0.96       200\n",
            "          26       0.99      0.84      0.91       200\n",
            "          27       0.92      0.85      0.88       200\n",
            "          28       0.89      0.79      0.84       200\n",
            "          29       0.84      1.00      0.91       200\n",
            "          30       0.83      0.88      0.85       200\n",
            "          31       0.92      0.87      0.89       200\n",
            "          32       0.98      0.94      0.96       200\n",
            "          33       0.98      1.00      0.99       200\n",
            "          34       0.93      1.00      0.96       200\n",
            "          35       0.99      0.81      0.89       200\n",
            "          36       0.95      1.00      0.98       200\n",
            "          37       0.91      0.92      0.91       200\n",
            "          38       0.93      0.98      0.96       200\n",
            "          39       0.87      1.00      0.93       200\n",
            "          40       1.00      0.90      0.95       200\n",
            "          41       0.92      0.86      0.89       200\n",
            "          42       0.86      0.65      0.74      1000\n",
            "          43       0.90      0.82      0.86       200\n",
            "          44       0.96      0.71      0.82       200\n",
            "          45       0.99      1.00      1.00       200\n",
            "          46       0.92      0.94      0.93       200\n",
            "          47       0.96      1.00      0.98       200\n",
            "          48       0.72      0.85      0.78       200\n",
            "          49       0.91      0.99      0.95       200\n",
            "          50       0.78      1.00      0.88       200\n",
            "          51       0.98      0.80      0.88       200\n",
            "          52       0.99      1.00      1.00       200\n",
            "          53       0.97      0.88      0.92       200\n",
            "          54       0.90      0.87      0.88       200\n",
            "          55       0.91      0.80      0.85       200\n",
            "          56       0.89      0.92      0.90       200\n",
            "          57       1.00      1.00      1.00       200\n",
            "          58       0.92      0.94      0.93       200\n",
            "          59       0.96      1.00      0.98       200\n",
            "          60       0.71      0.90      0.79       200\n",
            "          61       0.98      0.91      0.94       200\n",
            "          62       0.96      0.99      0.98       200\n",
            "          63       1.00      1.00      1.00       200\n",
            "          64       0.97      1.00      0.99       200\n",
            "          65       0.80      0.82      0.81       200\n",
            "          66       0.97      1.00      0.99       200\n",
            "          67       0.98      0.88      0.92       200\n",
            "          68       0.87      0.80      0.83       200\n",
            "          69       0.77      0.91      0.83       200\n",
            "          70       0.99      0.84      0.91       200\n",
            "          71       0.92      0.85      0.89       200\n",
            "          72       0.96      0.88      0.92       200\n",
            "          73       0.98      0.90      0.94       200\n",
            "          74       0.96      0.95      0.96       200\n",
            "          75       0.97      0.78      0.86       200\n",
            "          76       0.89      0.74      0.81       200\n",
            "          77       0.88      0.94      0.91       200\n",
            "          78       0.96      0.92      0.94       200\n",
            "          79       0.96      0.80      0.87       200\n",
            "          80       0.94      1.00      0.97       200\n",
            "          81       0.99      0.99      0.99       200\n",
            "          82       0.92      0.86      0.89       200\n",
            "          83       0.92      1.00      0.96       200\n",
            "          84       0.95      0.76      0.84       200\n",
            "          85       0.87      0.86      0.87       200\n",
            "          86       0.94      0.90      0.92       200\n",
            "          87       0.80      0.90      0.84       200\n",
            "          88       0.97      0.93      0.95       200\n",
            "          89       0.99      1.00      1.00       200\n",
            "          90       0.71      0.86      0.78       200\n",
            "          91       0.84      0.76      0.80       200\n",
            "          92       1.00      1.00      1.00       200\n",
            "          93       0.77      0.92      0.84       200\n",
            "          94       0.85      0.99      0.92       200\n",
            "          95       0.89      0.99      0.94       200\n",
            "          96       0.93      0.95      0.94       200\n",
            "          97       0.93      0.97      0.95       200\n",
            "          98       1.00      1.00      1.00       200\n",
            "          99       0.94      0.92      0.93       200\n",
            "         100       0.92      0.94      0.93       200\n",
            "         101       0.93      0.81      0.87       200\n",
            "         102       0.92      0.90      0.91       200\n",
            "         103       0.88      0.89      0.88       200\n",
            "         104       0.86      0.98      0.92       200\n",
            "         105       0.93      0.79      0.86       200\n",
            "         106       0.89      0.94      0.91       200\n",
            "         107       0.94      0.81      0.87       200\n",
            "         108       0.92      0.93      0.92       200\n",
            "         109       0.99      0.85      0.92       200\n",
            "         110       0.94      0.90      0.92       200\n",
            "         111       0.96      0.87      0.91       200\n",
            "         112       0.88      0.99      0.93       200\n",
            "         113       0.94      1.00      0.97       200\n",
            "         114       0.85      0.85      0.85       200\n",
            "         115       0.77      0.94      0.85       200\n",
            "         116       0.95      0.97      0.96       200\n",
            "         117       0.96      0.97      0.97       200\n",
            "         118       0.95      0.99      0.97       200\n",
            "         119       1.00      0.99      0.99       200\n",
            "         120       0.97      0.93      0.95       200\n",
            "         121       0.93      0.86      0.89       200\n",
            "         122       0.93      0.96      0.95       200\n",
            "         123       0.84      0.97      0.90       200\n",
            "         124       0.74      1.00      0.85       200\n",
            "         125       0.94      0.77      0.84       200\n",
            "         126       0.87      0.91      0.89       200\n",
            "         127       0.53      0.87      0.66       200\n",
            "         128       0.99      1.00      0.99       200\n",
            "         129       0.92      1.00      0.96       200\n",
            "         130       0.87      1.00      0.93       200\n",
            "         131       0.97      0.93      0.95       200\n",
            "         132       1.00      0.98      0.99       200\n",
            "         133       0.94      0.91      0.92       200\n",
            "         134       0.86      0.92      0.89       200\n",
            "         135       0.84      1.00      0.91       200\n",
            "         136       0.92      0.82      0.87       200\n",
            "         137       0.93      0.99      0.96       200\n",
            "         138       0.97      0.93      0.95       200\n",
            "         139       0.86      0.91      0.88       200\n",
            "         140       0.95      0.98      0.97       200\n",
            "         141       0.97      0.89      0.93       200\n",
            "         142       0.99      0.95      0.97       200\n",
            "         143       0.80      0.90      0.85       200\n",
            "         144       0.67      0.99      0.80       200\n",
            "         145       0.83      0.86      0.84       200\n",
            "         146       0.98      1.00      0.99       200\n",
            "         147       0.88      0.93      0.91       200\n",
            "         148       0.94      0.90      0.92       200\n",
            "         149       0.87      0.85      0.86       200\n",
            "         150       0.98      0.88      0.93       200\n",
            "\n",
            "    accuracy                           0.91     31000\n",
            "   macro avg       0.91      0.91      0.91     31000\n",
            "weighted avg       0.91      0.91      0.90     31000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focusing on the out-of-scope label \n",
        "\n",
        "Label: 42       \n",
        "Precision: 0.86      \n",
        "Recall:0.65      \n",
        "F1-score:0.74      \n",
        "Support: 1000\n",
        "\n",
        "We use recall to evaluate out-of-scope since we are more interested in cases where such queries are predicted as in-scope, as this would mean a system gives the user a response that is completely wrong. Precision errors are less problematic as the fallback response will prompt the user to try again, or inform the user of the system’s scope of supported domains."
      ],
      "metadata": {
        "id": "h2rJMazRMWtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loss curves\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(10),train_gen_loss_history,'-',linewidth=3,label='Generator Loss')\n",
        "plt.plot(range(10),train_disc_loss_history,'-',linewidth=3,label='Discriminator Loss')\n",
        "plt.plot(range(10),val_loss_history,'-',linewidth=3,label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "JATlpI2dGRsX",
        "outputId": "a009dc7b-19e4-4cd1-a43d-598cb94b27a4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1bd16c8700>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1dnA8d+ZLTtZIKxB2VFIIEAAFYTEXVBRBBXRAq1QfRWqtlbt5tLNva51Q1AqQitVrAtqRSha0UIQCIvsIAEEAiQkZJ2Z8/5xJ7OEJIQkMzczeb585jP33PXhEp45OXPvc5XWGiGEEJHHYnYAQgghgkMSvBBCRChJ8EIIEaEkwQshRISSBC+EEBHKZnYA/tq1a6e7devWqG1PnDhBXFxc8wYUpuRcBJLzEUjOh08knIvc3NwCrXVqbctaVILv1q0bq1evbtS2y5cvJzs7u3kDClNyLgLJ+Qgk58MnEs6FUmpPXctkiEYIISKUJHghhIhQkuCFECJCtagxeCFE/aqqqsjPz6e8vLzR+0hMTGTz5s3NGFX4CqdzER0dTVpaGna7vcHbSIIXIozk5+eTkJBAt27dUEo1ah/FxcUkJCQ0c2ThKVzOhdaaI0eOkJ+fT/fu3Ru8nQzRCBFGysvLadu2baOTuwhPSinatm172r+5hX+Cdzlhz1d0/f6fZkciREhIcm+dGvPvHt5DNG4XPDMAju+jJ8DRuyClh9lRCSFEixDePXiLFTpm+NpbPjYvFiFaiYMHD3LjjTfSo0cPhgwZwrnnnsu7775rWjzLly/nq6++avI+rrjiimaKqOUI7wQP0Ocy3/SWj8yLQ4hWQGvN1VdfzahRo9i5cye5ubksXLiQ/Pz8oB7X6XTWuawxCb6+/UWSyErw36+EskLzYhEiwn3++ec4HA5uvfVW77wzzzyTmTNnAuByubjnnnsYOnQoAwYM4OWXXwZ8JQEmTJjAWWedxeTJk6l+mlxubi6jR49myJAhXHrppRw4cACA7Oxs7rzzTrKysnjmmWd4//33GT58OIMGDeKiiy7i4MGD7N69m5deeom//OUvZGZm8sUXX7B7924uuOACBgwYwIUXXsj3338PwNSpU7n11lsZPnw4v/zlLxv0912wYAEZGRmkp6dz7733ev+OU6dOJT09nYyMDP7yl78A8Oyzz9KvXz8GDBjADTfc0Axnu+nCewweoE0n6DwI9n8Lbids/wwyJpgdlRBB1+2+D4O2792PjK11/saNGxk8eHCd27322mskJiayatUqKioqGDFiBJdccgkA3377LRs3bqRz586MGDGC//73vwwfPpyZM2fy3nvvkZqayt///nd+/etfM2fOHAAqKyu99amOHTvG119/jVKK2bNn89hjj/Hkk09y6623Eh8fzy9+8QsArrzySqZMmcKUKVOYM2cOs2bNYvHixYBxmelXX32F1Wo95TnYv38/9957L7m5uSQnJ3PJJZewePFiunbtyr59+9iwYQMAhYVGp/KRRx5h165dREVFeeeZLfwTPECfy40ED7BliSR4IULk9ttv58svv8ThcLBq1So+/fRT1q9fz6JFiwAoKipi27ZtOBwOhg0bRlpaGgCZmZns3r2bpKQkNmzYwMUXXwwYveNOnTp593/99dd7p/Pz87n++us5cOAAlZWVdV4PvnLlSt555x0Abr755oDe+sSJExuU3AFWrVpFdnY2qalGocbJkyezYsUKfvvb37Jz505mzpzJ2LFjvR9gAwYMYPLkyVx99dVcffXVDTpGsAV1iEYptVsplaeUWquUalyZyIbo6zdMs/3f4KoK2qGEaM369+/PmjVrvO0XXniBpUuXcvjwYcAYo3/uuedYu3Yta9euZdeuXd4EGBUV5d3OarXidDrRWtO/f3/v+nl5eXz66afe9fxL+c6cOZM77riDvLw8Xn755UbdzdscpYGTk5NZt24d2dnZvPTSS9xyyy0AfPjhh9x+++2sWbOGoUOHtohx/lD04HO01gVBPULHAZRHtSW64giUF8H3X0P384N6SCHMVtcwyqk05e7NCy64gF/96le8+OKL3HbbbQCUlpZ6l1966aW8+OKLXHDBBdjtdrZu3UqXLl3q3F/fvn05fPgwK1eu5Nxzz6WqqoqtW7fSv3//k9YtKiry7uuNN97wzk9ISOD48ePe9nnnncfChQu5+eabmT9/Puef37hcMGzYMGbNmkVBQQHJycksWLCAmTNnUlBQgMPh4Nprr6Vv377cdNNNuN1u9u7dS05ODiNHjmThwoWUlJSQlJTUqGM3l8gYolGKI22H0WX/EqO9ZYkkeCGCQCnF4sWLueuuu3jsscdITU0lLi6ORx99FIBbbrmF3bt3M3jwYLTWpKamese/a+NwOFi0aBGzZs2iqKgIp9PJnXfeWWuCf/DBB5k4cSLJyclccMEF7Nq1CzDG3CdMmMB7773Hc889x3PPPce0adN4/PHHSU1NZe7cuQ36uy1dutQ7hATw9ttv88gjj5CTk4PWmrFjxzJu3DjWrVvHtGnTcLvdAPz5z3/G5XJx0003UVRUhNaaWbNmmZ7cAVT1N9lB2blSu4BjgAZe1lq/Uss6M4AZAB06dBiycOHCRh0rZt+XDN/2OABl0R35ZvhL0Erv+CspKSE+Pt7sMFqMSDofiYmJ9OrVq0n7cLlcDR6HjnThdi62b99OUVFRwLycnJxcrXVWbesHuwc/Umu9TynVHvi3Uuo7rfUK/xU8Sf8VgKysLN3Yp6usWFoJu+Og6gQx5T+Qnd4ZUvs2Nf6wFAlPqWlOkXQ+Nm/e3OTiWOFSYCsUwu1cREdHM2jQoAavH9QvWbXW+zzvh4B3gWHBOpbb6oCeOb4ZW5YE61BCCBEWgpbglVJxSqmE6mngEmBDsI4HQN8xvumtUrZACNG6BXOIpgPwrqcCmg14S2sd3Kzb+xJAARr2fgMnjkBc26AeUgghWqqgJXit9U5gYLD2X6v4VEgbCvn/A+2GbZ9C5qSQhiCEEC1F+Neiqanv5b7prTIOL4RovSI7wW9fCs4K82IRIgJZrVYyMzPp378/AwcO5Mknn/ReE7569WpmzZrV5GO89NJLzJs377S2Oe+88xp9vNdff539+/c3enswrtN/4oknmrSP5hYZNzr5Sz0Lks6Ewj1QWQK7v4ReF5odlRARIyYmhrVr1wJw6NAhbrzxRo4fP85DDz1EVlYWWVm1XpLdYE6nM6BaZUM1pSb866+/Tnp6Op07d27wNuFwDX3k9eCVkqtphAiR9u3b88orr/D888+jtQ54cMZ//vMfMjMzyczMZNCgQRQXFwPw6KOPkpGRwcCBA7nvvvuAk0sD+/eGs7Ozueuuu8jKyuLss89m1apVjB8/nt69e/Ob3/zGG0v1zWz1lSZ++OGHGTp0KOnp6cyYMQOtNYsWLWL16tVMnjyZzMxMysrKWLp0KYMGDSIjI4Mf//jHVFQYIwHdunXj3nvvZfDgwbz99tunPD9aa+655x5vaeG///3vABw4cIBRo0aRmZlJeno6X3zxRZ1liJsi8nrwYBQf++ZFY3rLErj8sVZ7V6uIYA8mNmqzBt3W82DRqdfx6NGjBy6Xi0OHDgXMf+KJJ3jhhRcYMWIEJSUlREdHs2TJEt577z2++eYbYmNjOXr0qHd9/9LADz74YMC+HA4Hq1ev5plnnmHcuHHk5uaSkpJCz549ueuuu2jbNvBqudpKE48cOZI77riD3/3ud4BRafLjjz/muuuu4/nnn+eJJ54gKyuL8vJypk6dytKlS+nTpw8/+tGPePHFF7nzzjsBaNu2bUDBtfq88847rF27lnXr1lFQUMDQoUMZNWoUb731Fpdeeim//vWvcblclJaWsnbt2lrLEDdF5PXgAc4cAVGeH/6ivXBwo7nxCNEKjRgxgrvvvptnn32WwsJCbDYbn332GdOmTSM2NhaAlJQU7/r+pYFruuqqqwDIyMigf//+dOrUiaioKHr06MHevXtPWr+6NLHFYvGWJgZYtmwZw4cPJyMjg88//5zNmzeftO2WLVvo3r07ffr0AWDKlCmsWOG7Ab++OGv68ssvmTRpElarlQ4dOjB69GhWrVrF0KFDmTt3Lg8++CB5eXkkJCTQo0cPbxnijz/+mDZt2jT4OHWJzARvtQeOu8tdrUIEzc6dO7FarbRv3z5g/n333cfs2bMpKytjxIgRfPfdd/Xup75SvtWlhi0WS0DZYYvFUmtZ3tpKE5eXl/N///d/LFq0iLy8PKZPn25ayeFRo0axYsUKunTpwtSpU5k3b16dZYibIjKHaMC4mmajUfSfrUtg9D3mxiNEczuNYRR/zVl/5fDhw9x6663ccccdqBrDoDt27CAjI4OMjAxWrVrFd999x8UXX8zDDz/M5MmTvUM0/r34YKpO5u3ataOkpIRFixZx5ZVXAkbJ4ervCPr27cvu3bvZvn07vXr14m9/+xujR49u1DHPP/98Xn75ZaZMmcLRo0dZsWIFjz/+OHv27CEtLY3p06dTUVHBmjVrGDNmzElliJsqchN8r4tAWUG7YF8uFB+EhA5mRyVE2CsrKyMzM5OqqipsNhs333wzd99990nrPf300yxbtgyLxUL//v25/PLLiYqKYu3atWRlZeFwOBgzZgx/+tOfQhJ3UlIS06dPJz09nY4dOzJ06FDvsurntcbExLBy5Urmzp3LxIkTcTqdDB06tMFX9fzhD3/g6aef9rb37t3LypUrGThwIEopHnvsMTp27Mgbb7zB448/jt1uJz4+nnnz5rFv376TyhA3VVDLBZ+urKwsXf0ly+mqtWLg3LGw50tj+spnYciUpgUYJiKpemJziKTzsXnzZs4+++wm7SPcKigGU7idi9r+/ZVSdZYLjswx+Gr+j/KTyyWFEK1MhCd4v+vhdyyDqjLzYhFCiBCL7ATftie07W1MO8tg53/MjUcIIUIoshM8SPExIUSr1coS/Cfg+YZaCCEiXeQn+LRhEJNsTBcfgANrzY1HCCFCJPITvNUGvS/1teVqGiEaLScnh08++SRg3tNPP81tt91W5zbZ2dneGjNjxoyptcZKQ0rtLl68mE2bNnnbv/vd7/jss89OJ/xa+RdIizSRn+Ah8HLJLR+ZF4cQYW7SpEksXLgwYN7ChQuZNKlhT0776KOPSEpKatSxayb4hx9+mIsuuqhR+2otWkeC73khWOzG9A95UJRvbjxChKkJEybw4YcfUllZCcDu3bvZv38/559/PrfddhtZWVn079+fBx54oNbtu3XrRkFBAQB//OMf6dOnDyNHjmTLli3edV599VWGDh3KwIEDufbaayktLeWrr77iX//6F/fccw+ZmZns2LGDqVOnsmjRIoB6y/s+8MADDB48mIyMjFPWw/G3YMECMjIySE9P59577wWos6Tvs88+S79+/RgwYAA33HDDaZ7V4IncUgX+ottAt5Gwc5nR3voxDG16IR8hzJTxRkbQ9p03Ja/W+SkpKQwbNowlS5Ywbtw4Fi5cyHXXXYdSij/+8Y+kpKTgcrm48MILWb9+PQMGDKh1P7m5uSxcuJC1a9fidDoZPHgwQ4YMAWD8+PFMnz4dgN/85je89tprzJw5k6uuuoorrriCCRMmBOzrVOV927Vrx5o1a/jrX//KE088wezZs0/599+/fz/33nsvubm5JCcnc8kll7B48WK6du1aa0nfRx55hF27dhEVFdUsZX6bS+vowUPg1TRSXVKIRvMfpvEfnvnHP/7B4MGDGTRoEBs3bgwYTqnpiy++4JprriE2NpY2bdp4ywEDbNiwgfPPP5+MjAzmz5/Pxo31l/s+VXnf8ePHAzBkyBBv2eBTWbVqFdnZ2aSmpmKz2Zg8eTIrVqyos6TvgAEDmDx5Mm+++SY2W8vpN7eeBN/Hbxx+1wqoKDEvFiHC2Lhx41i6dClr1qyhtLSUIUOGsGvXLp544gmWLl3K+vXrGTt2bKNK8YJR+Ov5558nLy+PBx54oNH7qVZdOri6bHBT1FXS98MPP+T2229nzZo1DB06tMnHaS4t56Mm2JLPhPb94dBGcFUawzVnX2l2VEI0Wl3DKKfS1AJb8fHx5OTk8OMf/9jbez9+/DhxcXEkJiZy8OBBlixZUm+Bt1GjRjF16lTuv/9+nE4n77//Pj/96U+98XXq1Imqqirmz59Ply5dgMCSvv6as7xvtWHDhjFr1iwKCgpITk5mwYIFzJw5k4KCgpNK+rrdbvbu3UtOTg4jR45k4cKFlJSUNPrL5ObUehI8GFfTHPL8urdliSR4IRpp0qRJXHPNNd6hmoEDBzJo0CDOOussunbtyogRI+rdfvDgwVx//fUMHDiQ9u3bB5Tu/f3vf8/w4cNJTU1l+PDh3qR+ww03MH36dJ599lnvl6sA0dHRjS7vW23p0qWkpaV522+//TaPPPIIOTk5aK0ZO3Ys48aNY926dSeV9HW5XNx0000UFRWhtWbWrFktIrlDpJcLril/Ncz2POkpth38YitYWvZT0RsjksrjNodIOh9SLrh5hdu5kHLB9ek8GOI8jxUrLTAeBCKEEBGqdSV4iwX6XOJry01PQogI1roSPATWiN8iZQtE+GlJw6oidBrz7976EnyPbLB6nrh+eDMc3WVmNEKclujoaI4cOSJJvpXRWnPkyBGio6NPa7vWdRUNgCMOeoyGbZ8a7a0fwzl1F0oSoiVJS0sjPz+fw4cPN3of5eXlp50oIlU4nYvo6OiAK30aovUleDDuaq1O8FuWSIIXYcNut9O9e/cm7WP58uUMGjSomSIKb5F+LoI+RKOUsiqlvlVKfRDsYzWY/12te/4L5UXmxSKEEEESijH4nwGbQ3CchmvTGTplGtNuJ2xvek1pIYRoaYKa4JVSacBY4NTl20ItoPiYXE0jhIg8Qb2TVSm1CPgzkAD8Qmt90mNTlFIzgBkAHTp0GFLzYQINVVJSQnx8fIPXjy/eQVbu3QBU2eL56rx56Ai5q/V0z0Wkk/MRSM6HTySci5ycnDrvZA3al6xKqSuAQ1rrXKVUdl3raa1fAV4Bo1RBY28pP+3b0fVo2PoEFO/H7ixhdI8oo2Z8BIikW/Obg5yPQHI+fCL9XARziGYEcJVSajewELhAKfVmEI93epSq8Sg/qREvhIgsQUvwWuv7tdZpWutuwA3A51rrm4J1vEbpIw8BEUJErtZ3J6u/7qPAHmtMH90BBdvMjUcIIZpRSBK81np5bV+wms4eDT0v8LWl+JgQIoK07h48yOWSQoiIJQm+96WAMqb3fg2lR00NRwghmosk+PhUSPNcQqrdvho1QggR5iTBQ41hGrmaRggRGSTBQ+DlktuXgrPSvFiEEKKZSIIHaH82JJ1hTFcWw54vzY1HCCGagSR48NzVKo/yE0JEFknw1frUKFsgj0QTQoQ5SfDVzhwBUW2M6aLv4dAmc+MRQogmkgRfzeaAXhf62nI1jRAizEmC9yfFx4QQEUQSvL/eF4PyPPRjXy6UHDI3HiGEaAJJ8P5iU+CMczwNDVs/MTUcIYRoCknwNdW8mkYIIcKUJPia/K+H37kMqsrNi0UIIZpAEnxN7XpB217GdFUp7FphbjxCCNFIkuBrE1B8TB4CIoQIT5Lga+N/ueTWj+WuViFEWJIEX5uuwyE6yZguPgAH1pobjxBCNIIk+NpYbdDnUl9bio8JIcKQJPi6+F8uuVUulxRChB9J8HXpdSFYbMb0gXVQtM/ceIQQ4jRJgq9LdCJ0G+lrb5VhGiFEeJEEX5+aV9MIIUQYkQRfn75+4/A7/wOVJ8yLRQghTpMk+Pokd4P2/YxpVwXsWGZqOEIIcTokwZ+KXE0jhAhTkuBPxb/42NZPwO02LxYhhDgNkuBPpcsQiEs1pk8cNh4EIoQQYUAS/KlYLNDb/65WKT4mhAgPkuAboq9cLimECD9BS/BKqWil1P+UUuuUUhuVUg8F61hB1zMHrFHG9KFNcGy3qeEIIURDBLMHXwFcoLUeCGQClymlzjnFNi2TIw66j/K1pfiYECIMBC3Ba0OJp2n3vMK3sHrAMI1cLimEaPmUDuLDLJRSViAX6AW8oLW+t5Z1ZgAzADp06DBk4cKFjTpWSUkJ8fHxTYi2flHlBZz79U8AcCsb/x0xD5ctLmjHa4pgn4twI+cjkJwPn0g4Fzk5Obla66zalgU1wXsPolQS8C4wU2u9oa71srKy9OrVqxt1jOXLl5Odnd24ABvq5VFGZUmACXMhfXxwj9dIITkXYUTORyA5Hz6RcC6UUnUm+JBcRaO1LgSWAZedat0WTYqPCSHCSIMSvFIqTill8Uz3UUpdpZSyn2KbVE/PHaVUDHAx8F1TAzaVf/GxrZ+Ay2leLEIIcQoN7cGvAKKVUl2AT4GbgddPsU0nYJlSaj2wCvi31vqDxgbaInTKhIROxnR5Iez9xtx4hBCiHg1N8EprXQqMB/6qtZ4I9K9vA631eq31IK31AK11utb64aYGazqlpPiYECJsNDjBK6XOBSYDH3rmWYMTUgvnf7nkFknwQoiWq6EJ/k7gfuBdrfVGpVQPjC9NW5/uo8Aea0wf2Q4F282NRwgh6tCgBK+1/o/W+iqt9aOeL1sLtNazghxby2SPgR45vrYM0wghWqiGXkXzllKqjVIqDtgAbFJK3RPc0FowGaYRQoSBhg7R9NNaHweuBpYA3TGupGmd+lwKKGP6+6+h9Kip4QghRG0amuDtnuverwb+pbWuIpzryjRVfHvjQSAA2gXbPzM3HiGEqEVDE/zLwG4gDlihlDoTOB6soMJCwDCNPARECNHyNPRL1me11l201mM8VSL3ADmn3DBEdhXt4tsT34b2oP4JfvtScFaG9vhCCHEKDf2SNVEp9ZRSarXn9SRGb95UJZUl3PfFfVz93tXMPzKfY+XHQnfw9v0g8QxjuuI47Plv6I4thBAN0NAhmjlAMXCd53UcmBusoBoq1h7LlqNbcGs3FbqCeZvmhe7gSsmj/IQQLVpDE3xPrfUDWuudntdDQI9gBtYQFmXhpwN/6m2/tfktCssLQxeAf/GxLUsgBKWXhRCioRqa4MuUUiOrG0qpEUBZcEI6PZeceQk9E3sCUOosDW0v/syR4Egwpgv3wKHNoTu2EEKcQkMT/K3AC0qp3Uqp3cDzwE/r3yQ0LMrCrQNv9bbf+u4tiiqKQnNwmwN6Xehry12tQogWpKFX0azzPDx7ADBAaz0IuCCokZ2Gi8+8mI72jgCcqDoR2l58wOWSMg4vhGg5TuuJTlrr4547WgHuDkI8jWK1WLks0Tce/tbmEPbie18CynMa81dByeHQHFcIIU6hKY/sU80WRTMYFDuI7ondASipKuFvm/4WmgPHpkDXczwNDds+Cc1xhRDiFJqS4FvUJSMWZeHWAb6x+Pmb54euF1/zahohhGgB6k3wSqlipdTxWl7FQOcQxdhgl3a7NKAX/+bmN0Nz4L5jfNM7Pofig6E5rhBC1KPeBK+1TtBat6nllaC1toUqyIayWqzMGDDD256/aT7HK0NQMqddb0g9y5iuKoV3poPbFfzjCiFEPZoyRNMiXd7tcrq16QZAcVUx8zfND9GBH8X7tcSu/8CXT4XmuEIIUYeIS/A1e/F/2/S30PTie2TDqF/42sv+BHu+Cv5xhRCiDhGX4AEu716jF785RL340ffBGecZ09oNi34CJ46E5thCCFFDRCZ4m8V2Ui++uLI4+Ae22uDa2RCTYrSL98Pi26RGjRDCFBGZ4MHoxZ+RYJTzLa4MYS8+sQtc85Kvve0TWPlCaI4thBB+IjbB2yy2gEqTIevFg/HM1nPv8LU/exDyc0NzbCGE8IjYBA8wpvsYby/+eOVx3tr8VugOfuED0HmwMe2ugkXToDxEN14JIQQRnuBrjsXP2zSPksqSEB3cARPmQFQbo124B/41U8bjhRAhE9EJHmBsj7F0TegKeHrx34WwF5/SHa561tfe9B6snhO64wshWrWIT/A2i43pGdO97Xmb5nGi6kToAuh/DWT92Nf++H74IS90xxdCtFoRn+ABruh5BWnxaQAUVRSx4LsFoQ3g0j9Bh3Rj2lUBb0+DihANFQkhWq1WkeDtFnvAWPzrG18PbS/eHgMT5oI9zmgf2QYf/aL+bYQQoomCluCVUl2VUsuUUpuUUhuVUj8L1rEa4oqeV9AlvgtgUi8+tQ+MfdLXXrcA1obw+wAhRKsTzB68E/i51rofcA5wu1KqXxCPV6+avfg3Nr5BaVVpaIPInAQDb/S1P/w5HN4a2hiEEK1G0BK81vqA1nqNZ7oY2Ax0CdbxGuLKnld6e/GFFYWh78UDjHkc2vY2pqtK4e2pUFUW+jiEEBEvJGPwSqluwCDgm1Acry52iz3gihpTevFR8TDxdbBGGe1DG+GTX4U2BiFEq6B0kG+8UUrFA/8B/qi1fqeW5TOAGQAdOnQYsnDhwkYdp6SkhPj4+FOu59ROfr/v9xx1HQVgXNI4Lkq8qFHHbIrO+5bQZ5uvZs3Gfr/kcPsRzbLvhp6L1kLORyA5Hz6RcC5ycnJytdZZtS0LaoJXStmBD4BPtNanfAJGVlaWXr16daOOtXz5crKzsxu07qKti3ho5UMApESnsGT8EmLtsY06bqNpbQzPbFpstKPawE9XGDdHNdHpnIvWQM5HIDkfPpFwLpRSdSb4YF5Fo4DXgM0NSe6hNK7nODrFdQLgaPlR/rHlH6EPQinjLtekM412xXGjXo2zMvSxCCEiUjDH4EcANwMXKKXWel5jTrVRKNitdm7JuMXbnrtxbujH4gGiE2HiXLDYjfb+b43Kk0II0QyCeRXNl1prpbUeoLXO9Lw+CtbxTtc1va6hY1xHwOjFv731bXMC6TIELn7I1/76BdiyxJxYhBARpVXcyVobuzXwipo5G+ZQ5jTpcsVz/g/6XOZrL74NivLNiUUIETFabYIHuLrX1QG9eFPG4sEYj7/6RWjjuU2g7Bj88xZwOc2JRwgREVp1gndYHdyS7jcWv2Gueb342BTjea7K80/y/UpY/mdzYhFCRIRWneABrul9DR1iOwBwpPwIb28xaSwe4MzzIMfvpqcvnoQdy8yLRwgR1lp9gndYHSddUVPuLDcvoJF3Q8j0UbcAABkvSURBVPfRnoaGd2ZA8UHz4hFChK1Wn+ABxvceT/vY9gAUlBWYd0UNgMUK41+FuFSjfeIQvDsD3G7zYhJChCVJ8Jzci5+zYY65vfiEDkaSRxntncvhyxZ1r5gQIgxIgvcY33s87WN8vfh/bvunuQH1zIHz7/a1l/0R9nxlXjxCiLAjCd4jyhrFjzN8z059Le81KlwVJkYEZP8Kup5jTGu3celk6VFzYxJChA1J8H4m9JlAaowx9n247DCLti4yNyCrDSa8BjHJRvv4PuMmqCBXABVCRAZJ8H6irFH8JOMn3vacvDnm9+IT04yboKpt/Ri+/qt58QghwoYk+Bqu7X2ttxd/qOwQ/9xq8lg8QN/LjXIG1f79AOzLNS8eIURYkARfQ7Qtmh+n+43Fb2gBY/EAFz0EnQcZ0+4qeHsalBeZG5MQokWTBF+LCX0m0C6mHQCHSg/xzraTHkQVejYHTJhjPBgEoHAP/GuWjMcLIeokCb4WJ/Xi816j0tUCHsSR0gOufMbX3rQYcueaF48QokWTBF+HiX0m0ja6LQAHSw/y7rZ3TY7II308DJnmay+5D37YYF48QogWSxJ8HWr24l/Ne7Vl9OIBLvsztO9vTLsqjEf9VZSYG5MQosWRBF+PiX0De/GLty82OSIPe4zxqL/qB4UXbIWP7jE3JiFEiyMJvh4xthimpfuGQ1pULz61L4x5wtde9xasXWBePEKIFkcS/ClM7DORlOgUAH448UPL6cUDZN4IA27wtT/8ORzeal48QogWRRL8KcTaY5nW39eLn503mypXlYkR+VEKxj4JbXsZ7aoTsGgalpZw3b4QwnSS4Bvgur7XeXvxB04cYPGOFtSLj4qHia+DNcpoH9xAzx1y6aQQQhJ8g8TaY5naf6q3PXt9C+rFA3TMgMv+5G122b8EvnoOKktNDEoIYTZJ8A10fd/rSY4yqjruP7Gf93a8Z3JENWT9BM6+ytf+9Dfw1Fnw8f1QsM28uIQQppEE30Cx9limpk/1tmfnzabK3YJ68UrBVc9B296+eeVFRuXJ57PgjSth42JoSb95CCGCShL8abih7w3eXvy+kn28v+N9kyOqISYJpi9lR48pkNwtcNmuFfD2FPhLf/j8D1CUb0qIQojQkQR/GmLtsUzpP8XbfmX9Ky2rFw8QncjeM8bDzG/hpn9C37Gg/P6ZSw7Cisfh6QxYMAm2fSYP9BYiQkmCP02TzppEUlQSYPTiP9jxgckR1cFigV4XwaS34M48GH0vxHf0Lddu2PIRzL8WnhsEXz4NJwrMi1cI0ewkwZ+mmr34l9e/3PJ68TUlpkHOr+CuDXDdPOg+OnD5sd3w2QPw1NnGc1/3rJQyxEJEAEnwjTDprEkkRiUCLbwXX5PVDv3GwZR/wR25cM7tEJ3kW+6qhLy3Ye5l8OJ58L9Xofy4efEKIZpEEnwjxNnjmNLP14t/Ne9VnG6niRE1QrtexrXzP//OeOZrl6zA5Yc2wUe/gCfPgvd/BgfWmxOnEKLRJME3kn8vfm/xXj7YGSa9+JrsMUZNm+lL4acrYPAUX5VKMMof5L4OL58Psy8yCppVlZkWrhCi4YKW4JVSc5RSh5RSEfk0inhHPD/q9yNv+5X1r4RfL76mTgPhqmeNXv3lj0PqWYHL81fB4luNsfpPfg1HdpgTpxCiQYLZg38duCyI+zfdjWfdSBuH8YzUvcV7+WjXRyZH1EyiE2H4DPi/r2HaEkifABa7b3nZMVj5PDw3GOaNg03/AleYf7gJEYGCluC11iuAo8Haf0tQsxf/8rqXw78X708pOPM8mPAa3L0ZLnwAks4IXGfncvjHzfB0Oiz7Mxzfb0qoQoiTKR3Ey+GUUt2AD7TW6fWsMwOYAdChQ4chCxcubNSxSkpKiI+Pb9S2TVHmLuOBfQ9Q5jbGpW9uezPD4oeFPA5/QT0X2kXK0W/pvP9j2h5ZjSLw50djoaDdUPZ3vpxjyQMDb7IyiVk/Gy2VnA+fSDgXOTk5uVrrrNqWmZ7g/WVlZenVq1c36ljLly8nOzu7Uds21UvrXuKFtS8AkBqTyu/O/R2j00ajlDIlnpCdi8LvIfcNWDMPThw6eXniGdBpAKT0MF5texrvCZ2NG7FCxMyfjZZIzodPJJwLpVSdCd4W6mAi0eSzJzNv0zyKK4s5XHaYmZ/PZHin4dyTdQ99U/qaHV7wJJ0BF/7WuEv2uw9g9RzY/YVvedH3xqsmWzQkd/ck/u6+xJ/SA9qkhTT5CxHJJME3gwRHAo+c/wj3fXEfxZXFAHxz4Buu++A6xvcezx2Zd9A2pq3JUQaRzQHp443X4S1Gol+7ACqKal/fWQ6HNxuvmqxRRqE0b4+/+oOgp3FHrsUa1L+KEJEkaAleKbUAyAbaKaXygQe01q8F63hmG5U2ig+v+ZC/rv0rb299G5d24dZuFm1dxMe7PmbGgBlMPnsyDqvD7FCDK7UvXP4oXPQgHNwER3d6Xjt806VH6t7eVQEFW4xXTVaHL/nXfCV2Bav0V4TwF7T/EVrrScHad0uVHJ3Mr8/5NTecdQOPr3qc/+7/LwAlVSU8lfsU/9jyD36e9XMuPONC08bnQ8YeA2lDjFdNZcfg6C6/5O95HdkBpfUUPHNVQsFW41WTxQ7JZ/p6+95x/x6oll4rSIggkS5PEPRM6slLF7/EF/lf8Pjqx9lVtAuA/JJ87lp+F1kdsvjl0F9ydtuzTY7UJDHJ0CUZugw+eVl5kSf5V/f4dxmJ/+jO2r/IreaugiPbjVcNowG+tIMjDqISjHfvK97z8mtHxfstq55fy3a2CP9tTIQ9SfCNoLXmeLmTfcfK2F9Yxv6iMvYVlnHoeAVKQZTNSpTNQpStHaNi/kxn9Rm5x/9OudsYn199cDXXf3A9w9pdytXdfkL72PZE2Sw4bBZjO7sVh9VClN1oO6yWyO/xV4tOhM6ZxqumiuIaPX6/6ZIf6t+vuwrKC41Xc7E6anwQ1PLBEOX3AWKLNgq+WR3GbxxWu69ttXvmOYyhJqvDs57Nt7zmtq3lZ0I0miT4Wjhdbg4WV7C/sIx9x4zkvb/Q976/sJySitO5oakXWO4iKnUp9uSVKOVGo/mm4GO+Pvg5lUdyqDw6ErS9zj14k7/N4v0Acfi37Rbvh8KxgnI+OLwOu1Vhs1iwWRV2qwWrRWG3KGxWzzzPMlv1PIuxns2znd2qjG08y2xWi3efAcv85lXvx2YxljfrB1NUglFOodPAk5dVlMCxXb6hnure/9Ed6OKDKILwUBNXJZRVGkNOZvBP/jU/HCw1Pjz81ul/9CgcfM34wlpZ/d4tNdoNnK8sNZbVbNcxH2XMU8ozjee+CeU3z395HfNOubyufUJ02QGjXDbqFNvUnEcDY6ttP6H7YG6VCb64vIr9heXepO1L3EZC/+F4Oe7mvj3AHUvFwSupPHYO0e0/xJbwHQDKWklU+0+wJ31DxaHLcRYPwPhJD1TpdFPpdFPc0OMdaDmP5FPV/3eV8rzXmEadtA7+7Vq2J2Ab/33Eo1QmkOn9v1ROGYmxdmIpI5YKYnUZMaqcGF1GrC4nhnLPsnJitOdFGTG6jBjPvGhdRqwuIxpjuxhdhjUYHxqnw+00Xqf5FUMqgDzbBYBzAL4J/XHdxk8u2vN/XSsL33e8mB4/XdCsx4m4BO9yaw4XV7CvsJR9niResyd+vLzp5QSi7Ra6JMXQOSmGtOQYOifG0CExGotSVDhdVDrdVDjdVFS5A9vONCqdA/mhaj07XQsoU/sAsDgKiUlbgK3qGxyFV+MsS6PCk9QrXeH9SD2tMe53DbipLrQPFDlcXoXx424D4pphjxoHTuIoI05VEEs5cZQTq8qJp9xoq+r3CuIow4ETO04cyokNF3ZP2+6ZtimXd52A5SpwXRtOHMrVDH8HYRYLmoD/A9rFsZLmr9Ia9gl+9hc72bT/OBt3l/Gbbz7nh6JynM3Q/U5NiKJzUgxdkqLpnBhDl+QYT9t4JcXamzj8MAinezLvbHuH5799nmMVxq/5TvtOnKlPcWWPK/nZ4J/RIa4Dbrem0lX9AeGiospI+tUfHtUfBNXL1+VtpHefs6hyu3G6NFUuN063xul9195lTpebKrfG5T/P7abKFbi+d57fPl1ufdI8//Wb/begFkVRiZ1K7Bwz5XNLe5O9HSeO6mnl+8Co68PDghur5+WdVm4s6MB5dUxbVC3z0A1c13cMo/+qsXjejbPqaxu/nWnvet55AdsY8yw1+sQqoF3fMWo7Tu3HqHW7OmLDb151bBZV/w9HdW++OYV9gv9040H+t7u6plnDPgGjbL7ed+ek6IDE3Tkpho6J0UTbg39Djc1i47q+13FZ98t4df2rvLn5TW+xsvd3vs9n33/GtPRpTO0/lRh7jCemusfpq8Uf3Ur20K5Bjv7UtNbe3rvW2vMOGs98/+ka61DPMm0sPHl/fsfEb9nXX3/DsOHDA/dvrOG3nWe/1dN++/X9fQKPRY248Fvfu8+Tjuk7jnfFkyf99qNPmnfyurrW+dSxj7y8PAYMyPDOU/6JpfZJo+3XoVEB82us57f05GWB4WkNbu850gH/hoHLjC3cNf5t3Nr3b+au+fNW49+rtuVbt22jT+/eAbF6Y/TMUIFN79/vpPVrWafGm3EOtTY+gLRGKeOH3QJ0TvJ7DkMzCfsE3zkp+qR5beMcRo870ZfE05Krp2NoG+doUVeltHG04edZP2din4k8lfsUS79fCkCZs4y/rv0r/9z6T+4ccidjuo/B0gKKdzVU9fi5p2VaHLviLHRv1xzDMpHBenAz2Wd1MDuMFmF55W6yz+tmdhhBE/YJfsKQrpzbsy0Fe7Zx+ejhdE6KCUnvOxjOaHMGT+c8zf8O/I/HVj3GlmPG3ZwHSw9y/xf3s2DzAu4Zeg+Z7Wu5hFAIIWoIn+5gHUb2bsf1Q8+gfzsrPVLjwza5+xvWaRh/v+LvPHTeQ7SN9tWwWV+wnpuX3MwvV/ySAyUHTIxQCBEOwj7BRyqrxcr43uP54JoP+En6T3BYfHdNLtm1hCsXX8lz3z5HaVWpiVEKIVoySfAtXLwjnjuH3Ml7V7/HJWde4p1f4arglfWvcMW7V7B4+2LcOrwvpRRCND9J8GEiLSGNJ7Of5PXLXqdf237e+YfLDvPb//6WSR9OIvdgrokRCiFaGknwYWZIhyEsGLuAP4z4A6kxqd75m45sYurHU7l7+d0UVMltikKICLiKpjWyKAvjeo3j4jMvZs6GOby+8XUqXBUA/HvPv1nGMt758B3SEtJIi0+ja0JXuiZ0JS0hjfax7cPqUkshRONJgg9jsfZY7hh0B9f2vpan1zzNR7s+AsCJk7yCPPIK8k7axmFx0CWhi5HwayT/LvFdiLadfF+BECI8SYKPAJ3iO/HoqEe58ewbeXzV46w7vK7OdSvdlewq2uWtUV9T+5j2Rs8/Ic2b+Ks/BJKjklvUDWJCiPpJgo8gA1MH8uaYN1n82WK6pHchvzifvcV7yS/J904XVtRfD/1Q2SEOlR1izaE1Jy2LtcUGJP3q3wDSEtLoFN8Ju+XUZRSEaM2cbielzlJKq0opqSzhhPMEJ6pOUFpVSofYDmSkZpx6J6dBEnwESrIlMbTjUIZ2HHrSsuLKYvKL88kv8SR/T+LfW7yXH078gEvXXaWw1FnKlmNbvHfY+rMqKx3jOgYk/05xnYi2RRNljcJhdRBljQqY9p9ntzS1eJvQWlPlrqLcVU6lq5Jyp+fdFfi+vXw7ZxSeQVJ0EomORKzyIPM6aa0pd5V7k/CJqhOUVJV4p084ffOrX6VVpZRUlRjTztKA+eWu8jqPNa7nOEnwomkSHAmc3fbsWh8XWOWu4oeSH9hbYiR+/+S/t3gvpc66b6pyaRf7Svaxr2Qf3xxoXIHtU30QOKwOoix1f0hE2WpZZoliU9kmrPlWTz154w/Kv2iUb77yFpiqsczzXj2vep2AZf77rGUbp9tJhavCeDkrfNP+r1rm15Woa86vcFUEFjKrxzPvPeONMzEqkeToZJKjko13v+mkqCRSolNIik4iJSqF5OjksPiexq3d3kRbXFnse68sCZi35egW/v3lvwMStH9CLnWW1tvpaU71/f9qLEnwwstusdO1TVe6tjm5EqXWmmMVx3zDPn7JP78kn0Ol9TwvtYGqE1pxwx9r0nBLm3+XkUCjKawopLCikF3U/r1MTTG2GJKjkkmKTgr8YKjlAyI5Kpk2UW1O68ott3YbPeXKEoqraiTl2uZVlQTO9yxr6IddMH7c6qJQxNnjiLXHEmePI84W522nt0tv9uNJghcNopQiJTqFlOgUBqQOOGl5ubOc/SX7vQl/b/FeCsoKqHBVeHuXNd/9p6vcp/lYIlEru8Xu+43G77ca71CY1c7+gv3oKM3R8qMcrzx+2scoc5ZR5ixj/4n9DVrfoiwkRSV5PxRSolNo42hDmbPMm5yPVx73Tp+oOtHw5BwCUdYoIwnbPEnZP0H7v2wnz4+1x3qTeJw9jhhbTEiHIiXBi2YRbYumR1IPeiT1aNT2bu0O+ADwH3ao7QPhdOYdLDhISkqKpza4pvoP4Ks7778soL56jW107fPr26Z6mc1iC0y+NRJwtDUah9VBtC0ah8XzbnX45vsvrzE/yhaFw+Jo0Hj68uXLyc7OBoxhuaKKIo6VH6OwopCj5UcpLC/kaIXxfqz8GMcqjhnvnunT/TB2azdHy49ytPwoFJ3Wpk0SY4shwZ5AvCOeeEe8b9oeT4IjgXh7PD/s+YGB/QaelKDj7fHE2mOJtceG9cUDkuBFi2BRFqJt0UEZ3/VPaCKQ3WKnXUw72sW0a9D6WmtOVJ3wJv26PhT8p4urTn8MJNYWG5iU/aZrS9TxDt90giOBOHscNsup09vyY8vJ7pV92vGFC0nwQogGU0p5E27XhIY9NazKVUVhRaHvN4GKYxyvOE6MLabW5Bxvj5cre5qJJHghRFDZrXZSY1NJjU099cqiWUlREiGEiFCS4IUQIkJJghdCiAglCV4IISKUJHghhIhQkuCFECJCSYIXQogIpfxvsTabUuowsKeRm7cD5GGkBjkXgeR8BJLz4RMJ5+JMrXWtNxm0qATfFEqp1VrrLLPjaAnkXASS8xFIzodPpJ8LGaIRQogIJQleCCEiVCQl+FfMDqAFkXMRSM5HIDkfPhF9LiJmDF4IIUSgSOrBCyGE8CMJXgghIlTYJ3il1GVKqS1Kqe1KqfvMjsdMSqmuSqllSqlNSqmNSqmfmR2T2ZRSVqXUt0qpD8yOxWxKqSSl1CKl1HdKqc1KqXPNjslMSqm7PP9PNiilFiilmv9xYiYL6wSvlLICLwCXA/2ASUqpfuZGZSon8HOtdT/gHOD2Vn4+AH4GbDY7iBbiGeBjrfVZwEBa8XlRSnUBZgFZWut0wArcYG5UzS+sEzwwDNiutd6pta4EFgLjTI7JNFrrA1rrNZ7pYoz/wF3Mjco8Sqk0YCww2+xYzKaUSgRGAa8BaK0rtdaF5kZlOhsQo5SyAbHAfpPjaXbhnuC7AHv92vm04oTmTynVDRgEfGNuJKZ6Gvgl4DY7kBagO3AYmOsZspqtlIozOyizaK33AU8A3wMHgCKt9afmRtX8wj3Bi1oopeKBfwJ3aq2Pmx2PGZRSVwCHtNa5ZsfSQtiAwcCLWutBwAmg1X5npZRKxvhtvzvQGYhTSt1kblTNL9wT/D7A/9HuaZ55rZZSyo6R3Odrrd8xOx4TjQCuUkrtxhi6u0Ap9aa5IZkqH8jXWlf/RrcII+G3VhcBu7TWh7XWVcA7wHkmx9Tswj3BrwJ6K6W6K6UcGF+S/MvkmEyjlFIYY6ybtdZPmR2PmbTW92ut07TW3TB+Lj7XWkdcD62htNY/AHuVUn09sy4ENpkYktm+B85RSsV6/t9cSAR+6WwzO4Cm0Fo7lVJ3AJ9gfAs+R2u90eSwzDQCuBnIU0qt9cz7ldb6IxNjEi3HTGC+pzO0E5hmcjym0Vp/o5RaBKzBuPrsWyKwbIGUKhBCiAgV7kM0Qggh6iAJXgghIpQkeCGEiFCS4IUQIkJJghdCiAglCV60Kkopl1Jqrd+r2e7mVEp1U0ptaK79CdFUYX0dvBCNUKa1zjQ7CCFCQXrwQgBKqd1KqceUUnlKqf8ppXp55ndTSn2ulFqvlFqqlDrDM7+DUupdpdQ6z6v6NnerUupVT53xT5VSMab9pUSrJwletDYxNYZorvdbVqS1zgCex6hECfAc8IbWegAwH3jWM/9Z4D9a64EYNV2q76DuDbygte4PFALXBvnvI0Sd5E5W0aoopUq01vG1zN8NXKC13ukp2PaD1rqtUqoA6KS1rvLMP6C1bqeUOgykaa0r/PbRDfi31rq3p30vYNda/yH4fzMhTiY9eCF8dB3Tp6PCb9qFfM8lTCQJXgif6/3eV3qmv8L3KLfJwBee6aXAbeB97mtiqIIUoqGkdyFamxi/SptgPKO0+lLJZKXUeoxe+CTPvJkYT0G6B+OJSNUVGH8GvKKU+glGT/02jCcDCdFiyBi8EHjH4LO01gVmxyJEc5EhGiGEiFDSgxdCiAglPXghhIhQkuCFECJCSYIXQogIJQleCCEilCR4IYSIUP8PPyt835Wev+cAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy curves\n",
        "plt.plot(range(10), val_accuracies,'-', linewidth=3, label='Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "5eoPABNlGTlX",
        "outputId": "634dc80c-9fed-411c-a6d0-1ef1b081fce7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1bda19a0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has a decreasing loss curve for the generator loss, discriminator loss as well as the validation loss.\n",
        "And a stable increasing accuracy curve for validation accuracy - converging at around 95% accuracy which is similar to the BERT-base model."
      ],
      "metadata": {
        "id": "tB-SVwtieEDs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBAjfaSdMj3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}