{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0RZOLSUUMxt"
      },
      "source": [
        "# 90-10 split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckgPl4yQ_4WD"
      },
      "source": [
        "This notebook contains the code, statistics, loss & accuracy curves for the 90-10 variation of splitting the labeled & unlabeled examples that the GAN-BERT model is training on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rItvsdM7Tgk1"
      },
      "source": [
        "## Importing necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOAcH6FiUDuA",
        "outputId": "af5bd0b4-0c58-4e4c-f9a4-78d780607479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.3.2\n",
            "  Downloading transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (2022.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2) (4.64.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.3.2) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2) (2022.12.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=080e93e198db86caef3bf65d17c49c03a9cf2cbe60841e8017df3f724370d351\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[K     |████████████████████████████████| 452 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 20.1 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzDP5FZslqii"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj6Wy23MpCLl"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdliPXikUQu3"
      },
      "outputs": [],
      "source": [
        "# Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjQpweKxUU5N",
        "outputId": "3ca4d37b-3c24-452a-88ac-909ce5b8b9c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjAspE2Ywj1X"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53cSfHTmpE2T"
      },
      "source": [
        "## Data Processing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjv1V6p_lB_-"
      },
      "outputs": [],
      "source": [
        "with open('data_full.json') as json_file:\n",
        "  data = json.load(json_file)\n",
        "  json_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LUP9UluWzmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt0alestlLv_",
        "outputId": "fdb3050a-1f6d-43c9-f706-3fda872fd93a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['oos_val', 'val', 'train', 'oos_test', 'test', 'oos_train'])\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "['what expression would i use to say i love you if i were an italian', 'translate']\n"
          ]
        }
      ],
      "source": [
        "print(data.keys())\n",
        "print(type(data['train']))\n",
        "print(type(data['train'][0]))\n",
        "print(data['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WSUak9ctLFF",
        "outputId": "5f8ca1b7-3f31-43d3-cbe4-6e30b7dd669a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151\n",
            "['UNK_UNK', 'translate', 'transfer']\n"
          ]
        }
      ],
      "source": [
        "intent_set = ['UNK_UNK'] # set of all intents\n",
        "for sent in data['train']:\n",
        "  if sent[1] not in intent_set:\n",
        "    intent_set.append(sent[1])\n",
        "\n",
        "print(len(intent_set))\n",
        "print(intent_set[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBGR2WbUtn9x"
      },
      "outputs": [],
      "source": [
        "intent_map = {}\n",
        "for (i, label) in enumerate(intent_set):\n",
        "  intent_map[label] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT1zkVrDnrpc"
      },
      "outputs": [],
      "source": [
        "def split_labeled(intent_df, labeled_prop,unlabeled_prop):\n",
        "  test_size = unlabeled_prop/ (labeled_prop + unlabeled_prop)\n",
        "  df_labeled, df_unlabeled = train_test_split(intent_df, stratify = intent_df['intent'], test_size = test_size, random_state = 42)\n",
        "\n",
        "  df_labeled = df_labeled.reset_index(drop=True)\n",
        "  df_unlabeled = df_unlabeled.reset_index(drop=True)\n",
        "  print(df_labeled.head())\n",
        "  print(df_unlabeled.head())\n",
        "  print(df_labeled.shape)\n",
        "  print(df_unlabeled.shape)\n",
        "\n",
        "  #add into 2 lists - labeled_examples & unlabeled_examples\n",
        "  labeled_examples = []\n",
        "  unlabeled_examples = []\n",
        "  for i in range(len(df_labeled)):\n",
        "    labeled_examples.append((df_labeled.loc[i, 'text'], df_labeled.loc[i, 'intent']))\n",
        "  \n",
        "  print(labeled_examples[:3])\n",
        "\n",
        "  for i in range(len(df_unlabeled)):\n",
        "    unlabeled_examples.append((df_unlabeled.loc[i, 'text'], 'UNK_UNK'))\n",
        "\n",
        "  print(unlabeled_examples[:3])\n",
        "\n",
        "  return labeled_examples, unlabeled_examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BKVdTNlIo4S2",
        "outputId": "7a4216dd-e918-40dd-8520-44113de3e0d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text     intent\n",
              "0  what expression would i use to say i love you ...  translate\n",
              "1  can you tell me how to say 'i do not speak muc...  translate\n",
              "2  what is the equivalent of, 'life is good' in f...  translate\n",
              "3  tell me how to say, 'it is a beautiful morning...  translate\n",
              "4  if i were mongolian, how would i say that i am...  translate"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cfb91abf-6647-4064-9382-7bee6e0fd2a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what expression would i use to say i love you ...</td>\n",
              "      <td>translate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>can you tell me how to say 'i do not speak muc...</td>\n",
              "      <td>translate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what is the equivalent of, 'life is good' in f...</td>\n",
              "      <td>translate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tell me how to say, 'it is a beautiful morning...</td>\n",
              "      <td>translate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>if i were mongolian, how would i say that i am...</td>\n",
              "      <td>translate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cfb91abf-6647-4064-9382-7bee6e0fd2a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cfb91abf-6647-4064-9382-7bee6e0fd2a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cfb91abf-6647-4064-9382-7bee6e0fd2a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "intent_train_df = pd.DataFrame(data['train'], columns = ['text', 'intent'])\n",
        "intent_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvAp6vVin1h7",
        "outputId": "d54d83c0-4c1b-47be-d6a1-3be6398a9e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text            intent\n",
            "0                i want to know when i was last paid            payday\n",
            "1  do you know how many rewards points are curren...   rewards_balance\n",
            "2                   on my to do list, add exercising  todo_list_update\n",
            "3                          help me roll over my 401k     rollover_401k\n",
            "4                         can i get a w2 form online                w2\n",
            "                                             text            intent\n",
            "0           what is the traffic nearest lexington           traffic\n",
            "1  how much is the minimum payment for power bill       min_payment\n",
            "2                     when do i pay the utilities          bill_due\n",
            "3  i'd like to know how many vacation days i have       pto_balance\n",
            "4  talk to me by calling me this name from now on  change_user_name\n",
            "(13500, 2)\n",
            "(1500, 2)\n",
            "[('i want to know when i was last paid', 'payday'), ('do you know how many rewards points are currently on my discover card', 'rewards_balance'), ('on my to do list, add exercising', 'todo_list_update')]\n",
            "[('what is the traffic nearest lexington', 'UNK_UNK'), ('how much is the minimum payment for power bill', 'UNK_UNK'), ('when do i pay the utilities', 'UNK_UNK')]\n"
          ]
        }
      ],
      "source": [
        "#1st Variation\n",
        "labeled_examples, unlabeled_examples = split_labeled(intent_train_df, labeled_prop=90,unlabeled_prop=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5BsjpLfCfJ5"
      },
      "outputs": [],
      "source": [
        "# validation \n",
        "val_examples = []\n",
        "for sent in data['val']:\n",
        "  val_examples.append((sent[0], sent[1]))\n",
        "\n",
        "# test\n",
        "test_examples = []\n",
        "for sent in data['test']:\n",
        "  test_examples.append((sent[0], sent[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4R1vrbQ0v-j"
      },
      "source": [
        "Tokenization / Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqxgPFoHzpFA"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3iMspP52z3-",
        "outputId": "7d4c9b31-4fe3-47c1-d68e-89c64a35c519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SEP] 102\n",
            "[CLS] 101\n",
            "[PAD] 0\n",
            "[UNK] 100\n"
          ]
        }
      ],
      "source": [
        "# Some of the common BERT tokens\n",
        "print(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\n",
        "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows we’re doing classification\n",
        "print(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\n",
        "print(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JveGKJpxRrX"
      },
      "outputs": [],
      "source": [
        "# tokenizer + dataloader - no balancing like ganbert - add if accuracy is off\n",
        "\n",
        "def dataloader(ip_examples, label_masks, intent_map, do_shuffle, balance):\n",
        "  # applying balance\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples  \n",
        "  num_labeled_examples = 0 \n",
        "  for label_mask in label_masks:\n",
        "    if label_mask: \n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(ip_examples)\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(ip_examples): \n",
        "    if label_mask_rate == 1 or not balance: #if all labeled or balancing is not needed\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "\n",
        "\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # sentences = []\n",
        "  # for idx, sent in enumerate(examples):\n",
        "  #   sentences.append((sent, label_masks[idx]))\n",
        "\n",
        "  # tokenization\n",
        "  for (text, label_mask) in examples:\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=40, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "    label_id_array.append(intent_map[text[1]])\n",
        "    label_mask_array.append(label_mask)\n",
        "\n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "    input_mask_array.append(att_mask)\n",
        "\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids) \n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(dataset, sampler = sampler(dataset), batch_size = 64) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlNzGGBuIp3A"
      },
      "source": [
        "The sentence with the max length = 33, hence for each sentence in a batch, padding is done to make them all of length 40."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2LiAUqWs8_-",
        "outputId": "fa75be7a-3320-4a99-8e33-afebaea0a369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-199-7b370dd84a6f>:56: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  label_mask_array = torch.tensor(label_mask_array)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# label masks - True for labeled, False for unlabeled\n",
        "lab_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "unlab_label_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "train_label_masks = np.concatenate([lab_label_masks,unlab_label_masks])\n",
        "\n",
        "# train examples - labeled + unlabeled\n",
        "train_examples = labeled_examples + unlabeled_examples\n",
        "\n",
        "# train dataloader \n",
        "train_dataloader = dataloader(train_examples, train_label_masks, intent_map, do_shuffle = True, balance = True)\n",
        "\n",
        "#val dataloader\n",
        "val_label_masks = np.ones(len(val_examples), dtype=bool)\n",
        "val_dataloader = dataloader(val_examples, val_label_masks, intent_map, do_shuffle = False, balance = False)\n",
        "\n",
        "#test dataloader\n",
        "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "test_dataloader = dataloader(test_examples, test_label_masks, intent_map, do_shuffle = False, balance = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0r3sGdQdzMy"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0hcfDZPOYkn"
      },
      "outputs": [],
      "source": [
        "# BERT\n",
        "transformer =  BertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD_-wMT0d_vz"
      },
      "outputs": [],
      "source": [
        "#Generator\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size, output_size, hidden_sizes, dropout_rate):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeIbz8kIeGXz"
      },
      "outputs": [],
      "source": [
        "# Discriminator\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_labels, dropout_rate):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apkWunKpF5ql"
      },
      "outputs": [],
      "source": [
        "noise_size = 100\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "hidden_size = config.hidden_size # BERT outputs a 768 embedding vector\n",
        "num_hidden_layers_gen = 1\n",
        "hidden_levels_gen = [hidden_size for i in range(0, num_hidden_layers_gen)]\n",
        "num_hidden_layers_disc = 1\n",
        "hidden_levels_disc = [hidden_size for i in range(0, num_hidden_layers_disc)]\n",
        "\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.2\n",
        "\n",
        "num_labels = len(intent_map)\n",
        "\n",
        "generator = Generator(noise_size=noise_size, #100\n",
        "                      output_size=hidden_size, #768\n",
        "                      hidden_sizes=hidden_levels_gen, #[768] \n",
        "                      dropout_rate=out_dropout_rate) #0.2\n",
        "\n",
        "discriminator = Discriminator(input_size=hidden_size, #768\n",
        "                              hidden_sizes=hidden_levels_disc, #[768]\n",
        "                              num_labels=num_labels, #151 - no oos\n",
        "                              dropout_rate=out_dropout_rate) #0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTkVAX72N4jO",
        "outputId": "5974946d-51a6-42e2-c648-d71ed075d570"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=768, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=768, out_features=768, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ],
      "source": [
        "# input dim - 100x768\n",
        "# output dim - 768x768\n",
        "generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsBxCphHN8V4",
        "outputId": "6931f9ee-9cbe-4527-8669-92578af36927"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (input_dropout): Dropout(p=0.2, inplace=False)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (logit): Linear(in_features=768, out_features=152, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ],
      "source": [
        "# input dim - 768x768\n",
        "# output dim - 768x152\n",
        "discriminator  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juw-fYOCOzSb",
        "outputId": "27dd02d9-ff4f-44af-be5f-8af5052c67aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ],
      "source": [
        "# input dim - 30522x768\n",
        "# output dim - 768x768\n",
        "transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqaGXwp7UCvj"
      },
      "outputs": [],
      "source": [
        "multi_gpu = True\n",
        "if torch.cuda.is_available():    \n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC7NdupTUSq3"
      },
      "outputs": [],
      "source": [
        "#models parameters\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "learning_rate_generator = 5e-5\n",
        "learning_rate_discriminator = 5e-5\n",
        "\n",
        "#optimizer\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v96GTfibVs7M"
      },
      "outputs": [],
      "source": [
        "#scheduler\n",
        "apply_scheduler = False\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "warmup_proportion = 0.1\n",
        "\n",
        "if apply_scheduler:\n",
        "  num_train_examples = len(train_examples)\n",
        "  num_train_steps = int(num_train_examples / batch_size * num_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn4H3BUqkLlk"
      },
      "source": [
        "## Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwzIwRaCWOiK",
        "outputId": "204a34f5-11cf-49f6-96a4-feee2d0cec6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:28.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:54.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:20.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:47.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss generator: 0.673\n",
            "  Average training loss discriminator: 5.346\n",
            "  Training epoch took: 0:02:36\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.542\n",
            "Saving... 0\n",
            "  Val Loss: 2.954\n",
            "  Val took: 0:00:08\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:20.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:46.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss generator: 0.769\n",
            "  Average training loss discriminator: 2.727\n",
            "  Training epoch took: 0:02:36\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.887\n",
            "Saving... 1\n",
            "  Val Loss: 1.091\n",
            "  Val took: 0:00:08\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:20.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:46.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss generator: 0.750\n",
            "  Average training loss discriminator: 1.405\n",
            "  Training epoch took: 0:02:36\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.948\n",
            "Saving... 2\n",
            "  Val Loss: 0.431\n",
            "  Val took: 0:00:08\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:20.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:46.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:12.\n",
            "\n",
            "  Average training loss generator: 0.737\n",
            "  Average training loss discriminator: 0.950\n",
            "  Training epoch took: 0:02:35\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.957\n",
            "Saving... 3\n",
            "  Val Loss: 0.267\n",
            "  Val took: 0:00:08\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:19.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:46.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:12.\n",
            "\n",
            "  Average training loss generator: 0.731\n",
            "  Average training loss discriminator: 0.830\n",
            "  Training epoch took: 0:02:35\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.962\n",
            "Saving... 4\n",
            "  Val Loss: 0.225\n",
            "  Val took: 0:00:08\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:20.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:46.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss generator: 0.730\n",
            "  Average training loss discriminator: 0.773\n",
            "  Training epoch took: 0:02:35\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.962\n",
            "Saving... 5\n",
            "  Val Loss: 0.202\n",
            "  Val took: 0:00:08\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:19.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:46.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:12.\n",
            "\n",
            "  Average training loss generator: 0.728\n",
            "  Average training loss discriminator: 0.749\n",
            "  Training epoch took: 0:02:35\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.959\n",
            "  Val Loss: 0.211\n",
            "  Val took: 0:00:07\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:19.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:46.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:12.\n",
            "\n",
            "  Average training loss generator: 0.728\n",
            "  Average training loss discriminator: 0.744\n",
            "  Training epoch took: 0:02:35\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.954\n",
            "  Val Loss: 0.246\n",
            "  Val took: 0:00:07\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:19.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:45.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:12.\n",
            "\n",
            "  Average training loss generator: 0.730\n",
            "  Average training loss discriminator: 0.767\n",
            "  Training epoch took: 0:02:35\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.944\n",
            "  Val Loss: 0.312\n",
            "  Val took: 0:00:07\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    235.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    235.    Elapsed: 0:00:53.\n",
            "  Batch   120  of    235.    Elapsed: 0:01:19.\n",
            "  Batch   160  of    235.    Elapsed: 0:01:45.\n",
            "  Batch   200  of    235.    Elapsed: 0:02:12.\n",
            "\n",
            "  Average training loss generator: 0.727\n",
            "  Average training loss discriminator: 0.761\n",
            "  Training epoch took: 0:02:35\n",
            "\n",
            "Running validation...\n",
            "  Accuracy: 0.949\n",
            "  Val Loss: 0.272\n",
            "  Val took: 0:00:07\n"
          ]
        }
      ],
      "source": [
        "# training & validation\n",
        "best_valid_loss = float('inf')\n",
        "model = '90_10'\n",
        "training_stats = []\n",
        "train_gen_loss_history = []\n",
        "train_disc_loss_history = []\n",
        "val_loss_history = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
        "  print('Training...')\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  # loss for each epoch\n",
        "  tr_g_loss = 0\n",
        "  tr_d_loss = 0\n",
        "\n",
        "  transformer.train()\n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "\n",
        "  for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every print_each_n_step batches.\n",
        "    if idx % 40 == 0 and not idx == 0:\n",
        "        # Calculate elapsed time in minutes.\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        \n",
        "        # Report progress.\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(idx, len(train_dataloader), elapsed))\n",
        "\n",
        "    b_input_ids = batch[0].to(device) #tensor of input ids of that batch\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    b_label_mask = batch[3].to(device)\n",
        "\n",
        "    real_batch_size = b_input_ids.shape[0] \n",
        "    # print('real_batch_size: ', real_batch_size) #size of each batch = 64\n",
        "\n",
        "    model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "    hidden_states = model_outputs[-1]\n",
        "    # print('hidden states', hidden_states.size()) #64x768 - 768 per sentence in batch\n",
        "\n",
        "    noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "    # print('noise', noise.size()) # 64x100 - 100 per sentence in batch \n",
        "    gen_rep = generator(noise)\n",
        "    # print('gen output', gen_rep.size()) #64x768 \n",
        "    # both transformer + gen output are 768 size\n",
        "\n",
        "    disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "    # print('disc input', disciminator_input.size()) # 64x768\n",
        "    features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "    # print('disc output', features.size()) #64x768\n",
        "    # print('2: ', logits.size()) #64x152\n",
        "    # print('3: ', probs.size()) #64x152\n",
        "\n",
        "    features_list = torch.split(features, real_batch_size) #split 64x768 into 32,32\n",
        "    D_real_features = features_list[0]\n",
        "    D_fake_features = features_list[1] \n",
        "\n",
        "    logits_list = torch.split(logits, real_batch_size)\n",
        "    D_real_logits = logits_list[0]\n",
        "    D_fake_logits = logits_list[1]  \n",
        "\n",
        "    probs_list = torch.split(probs, real_batch_size)\n",
        "    D_real_probs = probs_list[0]\n",
        "    D_fake_probs = probs_list[1]\n",
        "\n",
        "    #loss\n",
        "\n",
        "    #gen loss\n",
        "    g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + 1e-8))\n",
        "    g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "    g_loss = g_loss_d + g_feat_reg\n",
        "\n",
        "    #disc loss\n",
        "    logits = D_real_logits[:,0:-1]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    label2one_hot = torch.nn.functional.one_hot(b_labels, 151)\n",
        "    per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "    per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "    labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "    if labeled_example_count == 0:\n",
        "      D_L_Supervised = 0\n",
        "    else:\n",
        "      D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "\n",
        "    D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + 1e-8))\n",
        "    D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + 1e-8))\n",
        "    d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "    #optimization\n",
        "    gen_optimizer.zero_grad()\n",
        "    dis_optimizer.zero_grad()\n",
        "\n",
        "    g_loss.backward(retain_graph=True)\n",
        "    d_loss.backward()\n",
        "\n",
        "    gen_optimizer.step()\n",
        "    dis_optimizer.step()\n",
        "\n",
        "    tr_g_loss += g_loss.item()\n",
        "    tr_d_loss += d_loss.item()\n",
        "\n",
        "    if apply_scheduler:\n",
        "      scheduler_d.step()\n",
        "      scheduler_g.step()\n",
        "\n",
        "  avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "  avg_train_loss_d = tr_d_loss / len(train_dataloader)\n",
        "  \n",
        "  training_time = format_time(time.time() - t0)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss generator: {0:.3f}\".format(avg_train_loss_g))\n",
        "  print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "  print(\"  Training epoch took: {:}\".format(training_time))\n",
        "  train_gen_loss_history.append(avg_train_loss_g)\n",
        "  train_disc_loss_history.append(avg_train_loss_d)\n",
        "\n",
        "  #EVALUATION\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Running validation...\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  transformer.eval() \n",
        "  discriminator.eval()\n",
        "  generator.eval()\n",
        "\n",
        "  total_val_accuracy = 0\n",
        "   \n",
        "  total_val_loss = 0\n",
        "  nb_test_steps = 0\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels_ids = []\n",
        "\n",
        "  #loss\n",
        "  nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "  for idx, batch in enumerate(val_dataloader):\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():        \n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        _, logits, probs = discriminator(hidden_states)\n",
        "        ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "        filtered_logits = logits[:,0:-1]\n",
        "        # Accumulate the test loss.\n",
        "        total_val_loss += nll_loss(filtered_logits, b_labels)\n",
        "\n",
        "    # Accumulate the predictions and the input labels\n",
        "    _, preds = torch.max(filtered_logits, 1)\n",
        "    all_preds += preds.detach().cpu()\n",
        "    all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "\n",
        "  # Report the final accuracy for this validation run.\n",
        "  all_preds = torch.stack(all_preds).numpy()\n",
        "  all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "  val_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "  print(\"  Accuracy: {0:.3f}\".format(val_accuracy))\n",
        "  val_accuracies.append(val_accuracy)\n",
        "\n",
        "  # Calculate the average loss over all of the batches.\n",
        "  avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "  avg_val_loss = avg_val_loss.item()\n",
        "  if avg_val_loss < best_valid_loss:\n",
        "    best_valid_loss = avg_val_loss\n",
        "    torch.save(transformer.state_dict(), 'model_'+model+'.pt')\n",
        "    print(\"Saving...\", epoch)\n",
        "  test_time = format_time(time.time() - t0)\n",
        "    \n",
        "  print(\"  Val Loss: {0:.3f}\".format(avg_val_loss))\n",
        "  print(\"  Val took: {:}\".format(test_time))\n",
        "  val_loss_history.append(avg_val_loss)\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss generator': avg_train_loss_g,\n",
        "            'Training Loss discriminator': avg_train_loss_d,\n",
        "            'Val Loss': avg_val_loss,\n",
        "            'Val Accur.': val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Val Time': test_time\n",
        "        }\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnXPbu7bkN1P"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rBWWVtyiPDP",
        "outputId": "0ac0a8c4-4173-4a99-c08f-222d111a5ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running test...\n",
            "  Accuracy: 0.957\n",
            "  Test Loss: 0.238\n",
            "  Test took: 0:00:10\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "\n",
        "print(\"\")\n",
        "print(\"Running test...\")\n",
        "transformer.load_state_dict(torch.load('model_'+model+'.pt')) # loading the best performing model \n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "transformer.eval() \n",
        "discriminator.eval()\n",
        "generator.eval()\n",
        "\n",
        "total_test_accuracy = 0\n",
        "  \n",
        "total_test_loss = 0\n",
        "nb_test_steps = 0\n",
        "\n",
        "all_preds = []\n",
        "all_labels_ids = []\n",
        "\n",
        "#loss\n",
        "nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "for idx, batch in enumerate(test_dataloader):\n",
        "  b_input_ids = batch[0].to(device)\n",
        "  b_input_mask = batch[1].to(device)\n",
        "  b_labels = batch[2].to(device)\n",
        "\n",
        "  with torch.no_grad():        \n",
        "      model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "      hidden_states = model_outputs[-1]\n",
        "      _, logits, probs = discriminator(hidden_states)\n",
        "      filtered_logits = logits[:,0:-1]\n",
        "      # Accumulate the test loss.\n",
        "      total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "\n",
        "  # Accumulate the predictions and the input labels\n",
        "  _, preds = torch.max(filtered_logits, 1)\n",
        "  all_preds += preds.detach().cpu()\n",
        "  all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "\n",
        "# Report the final accuracy for this validation run.\n",
        "all_preds = torch.stack(all_preds).numpy()\n",
        "all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "# Calculate the average loss over all of the batches.\n",
        "avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "avg_test_loss = avg_test_loss.item()\n",
        "\n",
        "test_time = format_time(time.time() - t0)\n",
        "  \n",
        "print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "print(\"  Test took: {:}\".format(test_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw8-JkYbNuTd"
      },
      "source": [
        "### Loss & Accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYmvWfgDJGDO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "R8APdI2FI_sm",
        "outputId": "6f93eb5d-ad5e-473b-9c49-3c457d6db4fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6cf4c70d90>"
            ]
          },
          "metadata": {},
          "execution_count": 214
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(10),train_gen_loss_history,'-',linewidth=3,label='Generator Loss')\n",
        "plt.plot(range(10),train_disc_loss_history,'-',linewidth=3,label='Discriminator Loss')\n",
        "plt.plot(range(10),val_loss_history,'-',linewidth=3,label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.title(\"Split \"+ model)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "rZnmK6qTJtoU",
        "outputId": "fbc7926b-1bab-4d96-8cd3-d73f7548b601"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6cf4c75700>"
            ]
          },
          "metadata": {},
          "execution_count": 183
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(10), val_accuracies,'-', linewidth=3, label='Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.title(\"Split \"+ model)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptD8J0yrNp2W"
      },
      "source": [
        "### Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjkB0uezNxf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92052b1c-d06f-44e0-cf2d-e8916265f0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "\n",
        "stats = []\n",
        "stats.append(\n",
        "    {\n",
        "    'gen train loss history': train_gen_loss_history ,\n",
        "    'disc train loss history': train_disc_loss_history,\n",
        "    'val loss': val_loss_history,\n",
        "    'val accuracy': val_accuracies,\n",
        "    'test loss': avg_test_loss,\n",
        "    'test accuracy': test_accuracy\n",
        "    }\n",
        ")\n",
        "\n",
        "with open('stats_'+model+'best'+'.txt','w') as f_out:\n",
        "  f_out.write(str(stats[0]))\n",
        "  print('Done')\n",
        "  f_out.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH5SmKoldaRD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}